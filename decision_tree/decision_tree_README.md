# 决策树算法最小实例

一个完整的决策树算法示例,使用贷款审批数据进行违约预测,展示决策树的完整流程。

## 📁 文件说明

- **decision_tree_sample.csv** - 贷款申请数据文件(100个样本)
- **decision_tree_example.py** - 决策树操作脚本
- **decision_tree_README.md** - 本说明文档

## 📊 数据说明

CSV文件包含6列(5个特征 + 1个目标):

### 特征变量
- `年龄` - 申请人年龄: 22-42岁
- `收入(万元)` - 年收入: 5-40万元
- `工作年限` - 工作年限: 0-20年
- `信用卡额度(万元)` - 信用卡额度: 1-30万元
- `负债率` - 负债率: 0.05-0.60

### 目标变量
- `违约记录` - 是否违约
  - 0 = 正常(未违约)
  - 1 = 违约

### 数据特点
- ✅ **100个样本**: 充足的数据量
- ✅ **5个特征**: 多维度申请人信息
- ✅ **二分类**: 经典分类问题
- ✅ **真实场景**: 金融风控应用

## 🎯 什么是决策树?

**决策树(Decision Tree)**是一种树形结构的机器学习算法,通过一系列规则对数据进行分类或回归。

### 核心思想

> 通过一系列"是/否"的问题,将数据不断分割,直到能够做出准确的预测。

### 决策树结构

```
                    [根节点]
                       |
          负债率 ≤ 0.35?
           /          \
         是            否
        /              \
    [内节点1]        [内节点2]
        |              |
  收入 ≤ 15万?    工作年限 ≤ 5年?
    /    \           /    \
[叶子] [叶子]    [叶子] [叶子]
 正常   违约      正常   违约
```

**节点类型**:
- **根节点**: 树的顶端,包含所有样本
- **内节点**: 中间节点,包含判断条件
- **叶子节点**: 树的末端,输出预测结果

### 算法特点

| 特点 | 说明 |
|------|------|
| **非参数化** | 不对数据分布做假设 |
| **可解释性强** | 可以可视化为树状图 |
| **不需要特征缩放** | 对特征尺度不敏感 |
| **自动特征选择** | 通过信息增益自动选择重要特征 |
| **易于理解** | 类似人类的决策过程 |

## 🚀 使用方法

### 1. 安装依赖

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

### 2. 运行脚本

```bash
python decision_tree_example.py
```

## 📈 输出内容

### 1. 控制台输出示例

```
决策树算法示例 - 贷款违约预测
======================================================================
数据预览:
   年龄  收入  工作年限  信用卡额度  负债率  违约记录
0   25     8       1         2    0.10       0
1   28    12       3         5    0.20       0
...

违约情况统计:
0    50
1    50
违约率: 50.00%

======================================================================
步骤1: 分析树深度对性能的影响
======================================================================
正在测试不同树深度的性能...
深度= 2 | 训练: 75.00% | 测试: 70.00% | 交叉验证: 72.50%
深度= 3 | 训练: 85.00% | 测试: 80.00% | 交叉验证: 78.75%
深度= 4 | 训练: 92.50% | 测试: 85.00% | 交叉验证: 81.25%
...

======================================================================
步骤2: 训练决策树模型 (最佳深度=4)
======================================================================
模型训练完成!

======================================================================
步骤3: 模型评估
======================================================================
准确率: 85.00%

混淆矩阵:
[[9 1]
 [2 8]]

分类报告:
              precision    recall  f1-score   support
        正常       0.82      0.90      0.86        10
        违约       0.89      0.80      0.84        10
    accuracy                           0.85        20
   macro avg       0.85      0.85      0.85        20
weighted avg       0.85      0.85      0.85        20

AUC值: 0.9200

======================================================================
决策树规则:
======================================================================
|--- 负债率 <= 0.35
|   |--- 收入 <= 15.00
|   |   |--- 违约: 1.0
|   |--- 收入 >  15.00
|   |   |--- 正常: 1.0
|--- 负债率 >  0.35
|   |--- 工作年限 <= 5.00
|   |   |--- 违约: 1.0
|   |--- 工作年限 >  5.00
|   |   |--- 正常: 1.0
======================================================================

特征重要性:
  负债率              : 0.4500
  工作年限            : 0.3000
  收入                : 0.1500
  年龄                : 0.0500
  信用卡额度          : 0.0500

示例预测:
======================================================================

申请人 1 (低风险):
  年龄: 30 岁
  收入: 15 万元
  工作年限: 5 年
  信用卡额度: 8 万元
  负债率: 20.0%
  预测结果: 正常
  正常概率: 85.00%
  违约概率: 15.00%
  决策路径: 经过 3 个节点
```

### 2. 可视化图表

#### decision_tree_structure.png (决策树结构)

完整的决策树可视化:
- 方框: 节点
- 颜色: 类别(蓝色=正常,橙色=违约)
- 文字: 分割条件、样本数、基尼系数等
- 层次: 从上到下的决策流程

#### decision_tree_feature_importance.png (特征重要性)

水平条形图展示各特征的重要性:
- 负债率通常最重要
- 工作年限、收入次之
- 数值标签标注具体重要性值

#### decision_tree_roc_curve.png (ROC曲线)

- 蓝色线: 模型ROC曲线
- 红色虚线: 随机分类器基线
- 绿色点: 最佳阈值点
- AUC值标注

#### decision_tree_confusion_matrix.png (混淆矩阵)

热力图展示预测结果:
- 真阳性(TP): 正确预测为违约
- 假阳性(FP): 错误预测为违约
- 假阴性(FN): 错误预测为正常
- 真阴性(TN): 正确预测为正常

#### decision_tree_depth_analysis.png (深度分析)

三条曲线展示不同深度的影响:
- 红色线: 训练集准确率(深度越大越高)
- 蓝色线: 测试集准确率(有最佳值)
- 绿色线: 交叉验证准确率
- 橙色虚线: 标注最佳深度

**观察要点**:
- 深度太小: 欠拟合
- 深度太大: 过拟合
- 选择测试准确率最高的深度

#### decision_tree_boundary.png (决策边界)

**左图 - 决策边界**:
- 背景色: 决策区域
- 数据点: 样本分布
- 可视化2维特征空间的分割

**右图 - 分割规则**:
- 水平/垂直线: 决策边界
- 展示树如何递归分割空间

#### decision_tree_principle.png (决策树原理)

完整的决策树示意图:
- 根节点、内节点、叶子节点
- 分支条件和标签
- 基尼系数、样本数等统计信息

#### decision_tree_feature_distributions.png (特征分布)

2×3子图展示5个特征的分布:
- 绿色: 正常客户
- 红色: 违约客户
- 可以看出哪些特征区分度高

## 🔑 核心概念详解

### 1. 信息增益与基尼系数

决策树需要选择最优的分割特征,使用两种主要指标:

#### 信息增益(基于熵)

```
熵(H) = -Σ pᵢ log₂(pᵢ)

信息增益 = H(父节点) - Σ[(nᵢ/n) × H(子节点ᵢ)]
```

- 熵越大,数据的不确定性越大
- 信息增益越大,分割效果越好

#### 基尼系数 (Gini Impurity)

```
基尼系数 = 1 - Σ pᵢ²
```

- sklearn默认使用基尼系数
- 计算更快,效果相近
- 范围[0, 0.5],越小越好

**示例**:
```
节点A: [50个正常, 50个违约]
基尼系数 = 1 - (0.5² + 0.5²) = 0.5

节点B: [90个正常, 10个违约]
基尼系数 = 1 - (0.9² + 0.1²) = 0.18

节点B更"纯",分割效果更好!
```

### 2. 树的构建过程

**ID3算法** (使用信息增益):
```
1. 计算所有特征的信息增益
2. 选择信息增益最大的特征作为根节点
3. 根据该特征的值分割数据
4. 对每个子节点递归执行步骤1-3
5. 停止条件:
   - 所有样本属于同一类
   - 没有更多特征可用
   - 达到最大深度
   - 节点样本数太少
```

**CART算法** (sklearn使用):
- 使用基尼系数
- 只进行二分(是/否)
- 可以用于分类和回归

### 3. 停止条件(剪枝)

#### 预剪枝(Pre-pruning)

在树生长过程中停止:
```python
max_depth=5              # 最大深度
min_samples_split=10     # 节点最小样本数
min_samples_leaf=5       # 叶子最小样本数
```

#### 后剪枝(Post-pruning)

树完全生长后再修剪:
- 成本复杂度剪枝(CCP)
- 降低了错误率修剪(REP)

### 4. 过拟合与欠拟合

| 树深度 | 训练准确率 | 测试准确率 | 状态 |
|--------|-----------|-----------|------|
| 太浅(2-3) | 低 | 低 | 欠拟合 |
| 适中(4-6) | 较高 | 较高 | 最佳 ✅ |
| 太深(>10) | 很高(接近100%) | 低 | 过拟合 |

**解决方法**:
- 限制树的深度
- 增加最小样本数
- 使用随机森林
- 交叉验证选择参数

## 💡 代码结构

```python
load_data()                      # 加载CSV数据
prepare_data()                   # 准备特征和目标变量
train_decision_tree()            # 训练决策树模型
evaluate_model()                 # 评估模型性能
visualize_tree_structure()       # 可视化树结构
print_tree_rules()               # 打印决策规则
visualize_feature_importance()   # 特征重要性
visualize_roc_curve()            # ROC曲线
visualize_confusion_matrix()     # 混淆矩阵
visualize_depth_impact()         # 深度分析
visualize_decision_boundary()    # 决策边界
visualize_tree_principle()       # 决策树原理
visualize_feature_distributions() # 特征分布
main()                           # 主函数
```

## 🎓 学习要点

1. **树结构**: 理解根节点、内节点、叶子节点
2. **分割准则**: 掌握信息增益和基尼系数
3. **过拟合**: 理解如何通过剪枝防止过拟合
4. **特征重要性**: 理解如何评估特征贡献
5. **可解释性**: 决策树的最大优势

## 🔧 参数调优指南

### 关键参数

```python
from sklearn.tree import DecisionTreeClassifier

# 1. max_depth: 树的最大深度
model = DecisionTreeClassifier(max_depth=5)  # 推荐3-6

# 2. min_samples_split: 分割节点所需的最小样本数
model = DecisionTreeClassifier(min_samples_split=10)  # 推荐10-20

# 3. min_samples_leaf: 叶子节点的最小样本数
model = DecisionTreeClassifier(min_samples_leaf=5)  # 推荐5-10

# 4. criterion: 分割准则
model = DecisionTreeClassifier(criterion='gini')   # 基尼系数(默认)
model = DecisionTreeClassifier(criterion='entropy') # 信息增益

# 5. splitter: 分割策略
model = DecisionTreeClassifier(splitter='best')    # 最佳分割(默认)
model = DecisionTreeClassifier(splitter='random')  # 随机分割
```

### 网格搜索

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 4, 5, 6, 7],
    'min_samples_split': [5, 10, 15, 20],
    'min_samples_leaf': [2, 5, 10],
    'criterion': ['gini', 'entropy']
}

grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

print(f"最佳参数: {grid.best_params_}")
print(f"最佳分数: {grid.best_score_}")
```

## 📚 扩展应用

### 1. 决策树回归

```python
from sklearn.tree import DecisionTreeRegressor

# 预测连续值(如房价)
regressor = DecisionTreeRegressor(max_depth=5)
regressor.fit(X_train, y_train)
prediction = regressor.predict(X_test)
```

### 2. 导出决策规则

```python
from sklearn.tree import export_text

# 文本形式
rules = export_text(model, feature_names=feature_names)
print(rules)
```

### 3. 可视化决策树

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(model,
         feature_names=feature_names,
         class_names=['正常', '违约'],
         filled=True,
         rounded=True)
plt.savefig('tree.png')
```

### 4. 获取决策路径

```python
# 获取样本的决策路径
sample = X_test[0:1]
path = model.decision_path(sample)

# 获取应用的特征
feature = model.decision_path(sample)
n_nodes = model.tree_.node_count
children_left = model.tree_.children_left
children_right = model.tree_.children_right
feature = model.tree_.feature
threshold = model.tree_.threshold
```

## ⚠️ 常见问题

**Q: 决策树和逻辑回归的区别?**
A:
- 决策树: 非线性,可处理复杂关系,可解释性强
- 逻辑回归: 线性,简单快速,输出概率

**Q: 如何防止过拟合?**
A:
- 限制树深度(max_depth)
- 增加最小样本数(min_samples_leaf)
- 使用剪枝技术
- 使用集成方法(随机森林)

**Q: 决策树需要特征标准化吗?**
A: **不需要!** 决策树基于分割点,不受特征尺度影响。

**Q: 如何选择分割准则?**
A:
- 基尼系数(gini): 默认,计算快
- 信息增益(entropy): 稍慢,理论上更优
- 实际效果相近,通常使用默认值

**Q: 决策树能处理缺失值吗?**
A: sklearn的DecisionTreeClassifier不支持,需要:
- 填充缺失值
- 使用XGBoost/LightGBM等(支持缺失值)

**Q: 决策树对异常值敏感吗?**
A: 不敏感。决策树基于排序,不受异常值影响。

**Q: 如何解释决策树的结果?**
A:
- 查看树结构图
- 打印决策规则
- 查看特征重要性
- 追踪决策路径

**Q: 决策树的时间复杂度?**
A:
- 训练: O(n×m×log n)
  - n: 样本数
  - m: 特征数
- 预测: O(log n) - 非常快!

## 📊 算法复杂度对比

| 指标 | 决策树 | 逻辑回归 | SVM | KNN |
|------|--------|----------|-----|-----|
| **训练时间** | 快 | 很快 | 慢 | 无 |
| **预测时间** | 快 | 快 | 快 | 慢 |
| **是否需要缩放** | 否 | 是 | 是 | 是 |
| **处理非线性** | 是 | 否 | 是 | 是 |
| **可解释性** | 高 | 中 | 低 | 中 |
| **异常值影响** | 小 | 大 | 大 | 大 |

## 🎯 实践建议

1. **数据预处理**:
   - 不需要标准化
   - 需要处理缺失值
   - 编码分类特征

2. **参数选择**:
   - 从max_depth=3-5开始
   - 使用交叉验证
   - 观察训练/测试曲线

3. **防止过拟合**:
   - 限制树的深度
   - 增加叶子节点最小样本数
   - 考虑使用随机森林

4. **特征工程**:
   - 决策树自动选择特征
   - 不需要太多人工特征工程
   - 注意高基数分类特征

5. **模型解释**:
   - 可视化树结构
   - 打印决策规则
   - 分析特征重要性

## 📝 决策树优缺点总结

### 优点 ✅
- 易于理解和解释
- 不需要特征缩放
- 自动特征选择
- 可处理数值和分类特征
- 对异常值不敏感
- 预测速度快
- 可视化友好

### 缺点 ❌
- 容易过拟合
- 不稳定(数据微小变化导致树结构大变)
- 容易陷入局部最优
- 难以处理线性关系
- 倾向于选择多值特征
- 需要大量数据才能学好

## 🔄 与其他算法对比

| 算法 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **决策树** | 可解释性强 | 容易过拟合 | 需要解释的场景 |
| **随机森林** | 精度高,不易过拟合 | 可解释性降低 | 追求精度 |
| **XGBoost** | 性能最强 | 参数多,易过拟合 | 竞赛、生产 |
| **逻辑回归** | 简单快速 | 只能线性关系 | 基准模型 |
| **SVM** | 理论完备 | 大数据慢 | 中小数据 |

## 📊 现在拥有的完整算法库

| 算法 | 类型 | 有监督/无监督 | 可解释性 | 文件 |
|------|------|--------------|---------|------|
| **线性回归** | 回归 | 有监督 | 高 | linear_regression.py |
| **逻辑回归** | 分类 | 有监督 | 高 | logistic_regression_multi.py |
| **SVM** | 分类 | 有监督 | 中 | svm_example.py |
| **KNN** | 分类 | 有监督 | 高 | knn_example.py |
| **K-Means** | 聚类 | 无监督 | 中 | kmeans_example.py |
| **决策树** | 分类 | 有监督 | **很高** ⭐ | decision_tree_example.py |

决策树是可解释性最强的算法之一! 它的树状结构和决策规则让我们能够完全理解模型的决策过程! 🌳✨

接下来可以学习集成方法(随机森林、XGBoost),它们基于决策树但性能更强! 🚀
