# K近邻(KNN)算法最小实例

一个完整的KNN算法示例,使用红酒品质数据进行分类,展示K值选择和距离度量的影响。

## 📁 文件说明

- **knn_sample.csv** - 红酒品质数据文件(81个样本)
- **knn_example.py** - KNN操作脚本
- **knn_README.md** - 本说明文档

## 📊 数据说明

CSV文件包含5列(4个特征 + 1个目标):

### 特征变量
- `含糖量(g/100ml)` - 糖分含量: 1.2-11.5 g/100ml
- `酸度(pH值)` - 酸碱度: 2.6-3.3 pH
- `密度(g/cm3)` - 酒液密度: 0.9920-1.0105 g/cm³
- `酒精含量(%)` - 酒精浓度: 8.5-13.9%

### 目标变量
- `品质` - 红酒品质等级
  - 0 = 普通品质
  - 1 = 优质品质

### 数据特点
- ✅ **81个样本**: 充足的训练数据
- ✅ **4个特征**: 化学成分指标
- ✅ **二分类**: 经典分类问题
- ✅ **真实场景**: 模拟红酒品质评估

## 🎯 什么是KNN?

**K近邻(K-Nearest Neighbors, KNN)**是一种简单而强大的监督学习算法。

### 核心思想

> "物以类聚,人以群分" - 预测一个样本的类别,看它最近的K个邻居属于哪一类。

### 算法流程

```
1. 计算待分类样本与所有训练样本的距离
2. 选择距离最近的K个样本
3. 统计这K个样本中各类别的数量
4. 将待分类样本归为数量最多的类别
```

### 算法特点

| 特点 | 说明 |
|------|------|
| **懒惰学习** | 没有显式的训练过程,预测时才计算 |
| **实例学习** | 直接使用训练数据,不需要构建模型 |
| **非参数化** | 不对数据分布做假设 |
| **简单直观** | 易于理解和实现 |

### 与其他算法对比

| 算法 | 类型 | 训练时间 | 预测时间 | 可解释性 |
|------|------|----------|----------|----------|
| **KNN** | 懒惰学习 | 极快 | 较慢 | 高 |
| **SVM** | 勤奋学习 | 较慢 | 极快 | 中 |
| **逻辑回归** | 勤奋学习 | 快 | 快 | 高 |
| **决策树** | 勤奋学习 | 中 | 快 | 高 |

## 🚀 使用方法

### 1. 安装依赖

```bash
pip install pandas numpy scikit-learn matplotlib
```

### 2. 运行脚本

```bash
python knn_example.py
```

## 📈 输出内容

### 1. 控制台输出示例

```
K近邻(KNN)算法示例 - 红酒品质预测
======================================================================
数据预览:
   含糖量  酸度   密度  酒精含量  品质
0   1.2  3.2  0.9920    8.5    0
1   1.5  3.0  0.9930    9.0    0
...

======================================================================
步骤1: 寻找最优K值
======================================================================
正在测试不同的K值...
K= 1 | 训练准确率: 100.00% | 测试准确率: 87.50% | 交叉验证: 85.42%
K= 3 | 训练准确率:  91.67% | 测试准确率: 90.62% | 交叉验证: 88.54%
K= 5 | 训练准确率:  90.62% | 测试准确率: 93.75% | 交叉验证: 90.62%
K= 7 | 训练准确率:  89.58% | 测试准确率: 92.19% | 交叉验证: 91.15%
K= 9 | 训练准确率:  87.50% | 测试准确率: 92.19% | 交叉验证: 91.67%
...
K=31 | 训练准确率:  81.25% | 测试准确率: 89.06% | 交叉验证: 88.02%

======================================================================
步骤3: 模型评估
======================================================================

K=1 模型:
  准确率: 87.50%
  AUC值: 0.9115

K=5 模型:
  准确率: 93.75%
  AUC值: 0.9694

K=11 模型:
  准确率: 92.19%
  AUC值: 0.9635

K=21 模型:
  准确率: 90.62%
  AUC值: 0.9519

最佳K值: K=5
准确率: 93.75%

示例预测:
======================================================================
样本1 (低含糖量红酒):
  含糖量: 2.5 g/100ml
  酸度: 3.0
  密度: 0.9945 g/cm³
  酒精含量: 9.0%
  预测品质: 普通品质
  普通品质概率: 80.00%
  优质品质概率: 20.00%
  最近5个邻居的平均距离: 2.3456
```

### 2. 可视化图表

#### knn_k_selection.png (K值选择曲线)

三条曲线对比不同K值的性能:
- **红色线**: 训练集准确率(K越小越高,过拟合)
- **蓝色线**: 测试集准确率(有最佳值)
- **绿色线**: 交叉验证准确率(更可靠)
- **橙色虚线**: 标注最佳K值
- **黄色星**: 最佳测试准确率点

**观察要点**:
- K=1时训练准确率100%,但测试准确率较低(过拟合)
- K增大时训练准确率下降,测试准确率先升后降
- 选择测试准确率最高的K值

#### knn_decision_boundary.png (决策边界可视化)

2×3子图展示不同K值(1,3,5,11,21,31)的决策边界:
- 背景色: 决策区域
- 红点: 普通品质
- 蓝点: 优质品质

**观察要点**:
- K=1: 决策边界非常复杂,过拟合
- K=5: 较平滑,泛化能力好
- K=31: 过于平滑,可能欠拟合

#### knn_principle.png (KNN原理图)

**左图 - KNN分类原理**:
- 红点: 普通品质样本
- 蓝点: 优质品质样本
- 黄色星: 待分类样本
- 绿色虚线圆圈: K=5的邻居范围
- 绿色虚线: 连接到最近5个邻居

**右图 - 距离度量示例**:
- 展示欧氏距离的计算
- 两点之间的直线距离
- 距离公式标注

#### knn_distance_metrics.png (距离度量对比)

四种距离度量的准确率对比:
- **欧氏距离**: 最常用,直线距离
- **曼哈顿距离**: 城市街区距离
- **切比雪夫距离**: 各维度最大距离
- **闵可夫斯基距离**: 推广的距离度量

#### knn_feature_distribution.png (特征分布)

2×2子图展示4个特征的分布:
- 红色直方图: 普通品质
- 蓝色直方图: 优质品质
- 可以看出哪些特征区分度高

## 🔑 核心概念详解

### 1. K值的选择

K值是KNN最重要的超参数,直接影响模型性能。

| K值 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **K太小** (如K=1) | 捕捉局部模式 | 对噪声敏感,过拟合 | 数据非常干净 |
| **K适中** (如K=5) | 平衡偏差和方差 | 需要调优 | **大多数场景** |
| **K太大** (如K=31) | 对噪声鲁棒 | 欠拟合,边界简单 | 数据噪声大 |

**K值选择经验**:
- 从K=√n开始(n为样本数)
- 通常选择奇数避免平局
- 通过交叉验证确定最优K
- 观察: K越小模型越复杂,K越大模型越简单

### 2. 距离度量

KNN的核心是距离计算,不同的距离度量会影响分类结果。

#### 欧氏距离 (最常用)

```
d(p, q) = √[(q₁-p₁)² + (q₂-p₂)² + ... + (qₙ-pₙ)²]
```

- 两点之间的直线距离
- 适合连续特征
- 对异常值敏感

#### 曼哈顿距离

```
d(p, q) = |q₁-p₁| + |q₂-p₂| + ... + |qₙ-pₙ|
```

- 城市街区距离
- 适合稀疏特征
- 对异常值较鲁棒

#### 切比雪夫距离

```
d(p, q) = max(|q₁-p₁| |q₂-p₂|, ..., |qₙ-pₙ|)
```

- 各维度最大距离
- 适合国际象棋国王移动

#### 闵可夫斯基距离 (推广形式)

```
d(p, q) = (|q₁-p₁|ᵖ + |q₂-p₂|ᵖ + ... + |qₙ-pₙ|ᵖ)^(1/p)
```

- p=2: 欧氏距离
- p=1: 曼哈顿距离
- p→∞: 切比雪夫距离

### 3. 决策规则

#### 多数投票

```python
# K=5的例子
邻居类别: [0, 0, 1, 1, 1]
计数: 类别0出现2次, 类别1出现3次
预测: 类别1 (多数)
```

#### 加权投票

```python
# 考虑距离的权重
邻居类别: [0, 0, 1, 1, 1]
邻居距离: [1.2, 1.5, 0.8, 1.0, 1.3]

权重 = 1 / 距离
类别0权重: 1/1.2 + 1/1.5 = 1.50
类别1权重: 1/0.8 + 1/1.0 + 1/1.3 = 3.27
预测: 类别1 (权重和最大)
```

### 4. 特征缩放的重要性

**为什么必须标准化?**

不同特征的尺度不同,会导致某些特征主导距离计算:

```
特征1: 含糖量 1-15 g/100ml
特征2: 密度 0.99-1.01 g/cm³

如果不标准化:
含糖量差值: 10 g/100ml → 距离贡献 ~100
密度差值: 0.02 g/cm³ → 距离贡献 ~0.0004
结果: 含糖量完全主导距离计算!
```

**解决方案**:
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # 标准化到均值0,方差1
```

## 💡 代码结构

```python
load_data()                    # 加载CSV数据
prepare_data()                 # 准备特征和目标变量
find_optimal_k()               # 寻找最优K值
train_knn_models()             # 训练不同K值的KNN
evaluate_model()               # 评估模型性能
visualize_k_selection()        # 可视化K值选择
visualize_decision_boundary()  # 可视化决策边界
visualize_knn_principle()      # KNN原理图
visualize_distances_comparison() # 距离度量对比
visualize_feature_distributions() # 特征分布图
main()                         # 主函数
```

## 🎓 学习要点

1. **K值选择**: 理解K对过拟合和欠拟合的影响
2. **距离度量**: 掌握不同距离的计算和应用
3. **特征缩放**: 理解为什么KNN必须标准化
4. **交叉验证**: 使用CV选择最优超参数
5. **懒惰学习**: 理解KNN的独特特点

## 🔧 参数调优指南

### 选择最优K值

```python
# 方法1: 交叉验证
from sklearn.model_selection import cross_val_score

k_range = range(1, 31, 2)
cv_scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    cv_scores.append(scores.mean())

best_k = k_range[np.argmax(cv_scores)]
```

```python
# 方法2: 网格搜索
from sklearn.model_selection import GridSearchCV

param_grid = {'n_neighbors': range(1, 31, 2),
              'weights': ['uniform', 'distance'],
              'metric': ['euclidean', 'manhattan']}

grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

print(f"最佳参数: {grid.best_params_}")
```

### 权重选项

```python
# uniform: 所有邻居权重相同
knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')

# distance: 距离越近权重越大
knn = KNeighborsClassifier(n_neighbors=5, weights='distance')
```

### 算法优化

```python
# brute: 暴力搜索(适合小数据)
knn = KNeighborsClassifier(algorithm='brute')

# kd_tree: KD树(适合低维数据)
knn = KNeighborsClassifier(algorithm='kd_tree')

# ball_tree: 球树(适合中高维数据)
knn = KNeighborsClassifier(algorithm='ball_tree')

# auto: 自动选择
knn = KNeighborsClassifier(algorithm='auto')
```

## 📚 扩展应用

### 1. 回归问题 (KNN回归)

```python
from sklearn.neighbors import KNeighborsRegressor

# 预测连续值(如房价)
knn_reg = KNeighborsRegressor(n_neighbors=5)
knn_reg.fit(X_train, y_train)
prediction = knn_reg.predict(X_test)
```

### 2. 加权KNN

```python
# 距离倒数作为权重
knn = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance',  # 使用距离加权
    metric='euclidean'
)
```

### 3. 半径邻域

```python
from sklearn.neighbors import RadiusNeighborsClassifier

# 固定半径而非固定K值
rnn = RadiusNeighborsClassifier(radius=1.0)
```

### 4. 近似最近邻

```python
# 大数据加速
from sklearn.neighbors import NearestNeighbors

# 使用近似算法加速
nn = NearestNeighbors(n_neighbors=5, algorithm='auto')
nn.fit(X_train)
```

## ⚠️ 常见问题

**Q: 为什么KNN叫"懒惰学习"?**
A: 因为KNN没有显式的训练过程,只是在预测时才计算距离,所以称为"懒惰"。

**Q: K值如何选择?**
A: 一般从√n开始(n为样本数),然后通过交叉验证微调。奇数K可以避免投票平局。

**Q: KNN必须标准化特征吗?**
A: **必须!** 不同尺度的特征会导致距离计算被大尺度特征主导。

**Q: KNN适合大数据吗?**
A: 不太适合。KNN预测时间复杂度O(n),大数据(>10万样本)会非常慢。

**Q: KNN能处理缺失值吗?**
A: 不能。需要在训练前填充或删除缺失值。

**Q: KNN对异常值敏感吗?**
A: 敏感。异常值会严重影响距离计算,建议预处理时去除异常值。

**Q: 加权投票一定比均匀投票好吗?**
A: 不一定。加权投票对噪声更敏感,需要根据数据特点选择。

**Q: 如何处理类别不平衡?**
A: 可以使用加权KNN或调整投票阈值,也可以对少数类过采样。

## 📊 算法复杂度对比

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| **训练** | O(1) | O(n) |
| **预测** | O(n×d) | O(1) |
| **暴力搜索** | O(n×d) | O(1) |
| **KD树构建** | O(n×d×log n) | O(n×d) |
| **KD树搜索** | O(d×log n) | O(1) |

- n: 样本数
- d: 特征维度

## 🎯 实践建议

1. **数据预处理**: 必须标准化特征
2. **K值选择**: 使用交叉验证,从√n开始
3. **距离度量**: 默认欧氏距离,高维考虑曼哈顿距离
4. **数据规模**: <10万样本适合,>10万考虑其他算法
5. **特征选择**: KNN对无关特征敏感,需要特征选择
6. **评估指标**: 使用交叉验证,避免偶然性

## 📝 KNN优缺点总结

### 优点 ✅
- 简单直观,易于理解和实现
- 无训练过程,添加新数据容易
- 适合多分类问题
- 对异常值相对鲁棒(当K较大时)
- 适合低维数据

### 缺点 ❌
- 预测速度慢,需要计算所有距离
- 需要存储全部训练数据
- 对特征缩放敏感
- 不适合高维数据(维度灾难)
- 对无关特征敏感
- 查询效率低

## 🔄 算法对比总结表

现在你拥有完整的机器学习算法系列:

| 算法 | 类型 | 核心思想 | 训练速度 | 预测速度 | 数据规模 |
|------|------|----------|----------|----------|----------|
| **线性回归** | 回归 | 最小二乘法 | 快 | 快 | 任意 |
| **逻辑回归** | 分类 | Sigmoid概率 | 快 | 快 | 任意 |
| **SVM** | 分类 | 最大化间隔 | 慢 | 快 | 中小 |
| **KNN** | 分类 | 近邻投票 | 无 | 慢 | 小 |
| **决策树** | 分类 | 信息增益 | 中 | 快 | 任意 |
| **随机森林** | 分类 | 集成学习 | 慢 | 中 | 任意 |
| **神经网络** | 分类 | 反向传播 | 慢 | 快 | 大 |

选择算法时考虑:
- 数据规模和维度
- 训练/预测时间要求
- 是否需要概率输出
- 是否需要可解释性
- 是否可以增量学习
