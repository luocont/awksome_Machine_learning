# K-Means聚类算法最小实例

一个完整的K-Means聚类示例,使用客户数据进行客户分群,展示聚类算法的完整流程。

## 📁 文件说明

- **kmeans_sample.csv** - 客户数据文件(100个样本)
- **kmeans_example.py** - K-Means操作脚本
- **kmeans_README.md** - 本说明文档

## 📊 数据说明

CSV文件包含5列(无标签,纯特征):

### 特征变量
- `年龄` - 客户年龄: 22-60岁
- `年收入(万元)` - 年收入: 5-52万元
- `年消费(万元)` - 年消费: 3-33万元
- `购物频率(次/月)` - 每月购物次数: 2-20次
- `在线时长(小时/周)` - 每周在线时长: 2-16小时

### 数据特点
- ✅ **100个样本**: 充足的数据量
- ✅ **5个特征**: 多维度客户信息
- ✅ **无监督学习**: 没有标签,需要自动发现分组
- ✅ **真实场景**: 模拟电商客户分群

## 🎯 什么是K-Means?

**K-Means聚类**是一种经典的无监督学习算法,用于将数据分成K个组(聚类)。

### 核心思想

> 将相似的样本聚集在一起,使同一聚类内的样本尽可能相似,不同聚类的样本尽可能不同。

### 算法流程

```
步骤1: 初始化
  随机选择K个点作为初始聚类中心

步骤2: 分配样本
  计算每个样本到各中心的距离
  将样本分配到最近的中心

步骤3: 更新中心
  计算每个聚类的新中心点
  (该聚类所有点的均值)

步骤4: 迭代
  重复步骤2-3
  直到中心点不再变化或达到最大迭代次数
```

### 算法特点

| 特点 | 说明 |
|------|------|
| **无监督学习** | 不需要标签,自动发现数据结构 |
| **划分式聚类** | 直接将数据划分为K个不相交的聚类 |
| **基于质心** | 每个聚类用中心点(质心)表示 |
| **迭代优化** | 通过迭代逐步优化聚类结果 |

### 与监督学习的区别

| 对比项 | 监督学习 (如KNN) | 无监督学习 (如K-Means) |
|--------|------------------|----------------------|
| **数据标签** | 需要标签 | 不需要标签 |
| **学习目标** | 预测新样本的标签 | 发现数据的内在结构 |
| **评估方式** | 准确率、精确率等 | 轮廓系数、惯性等 |
| **应用场景** | 分类、回归 | 聚类、降维 |

## 🚀 使用方法

### 1. 安装依赖

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

### 2. 运行脚本

```bash
python kmeans_example.py
```

## 📈 输出内容

### 1. 控制台输出示例

```
K-Means聚类算法示例 - 客户分群
======================================================================
数据预览:
   年龄  年收入  年消费  购物频率  在线时长
0   25      8      5        3        8
1   23      7      4        2        7
...

======================================================================
寻找最优K值...
======================================================================
K= 2 | 惯性:  395.42 | 轮廓系数: 0.4234 | CH指数:  185.67 | DB指数: 0.8234
K= 3 | 惯性:  278.35 | 轮廓系数: 0.5234 | CH指数:  245.89 | DB指数: 0.6123
K= 4 | 惯性:  215.67 | 轮廓系数: 0.5845 | CH指数:  287.34 | DB指数: 0.5456
K= 5 | 惯性:  178.23 | 轮廓系数: 0.6123 | CH指数:  298.45 | DB指数: 0.5234
...

最优K值推荐:
  轮廓系数推荐: K=4
  CH指数推荐: K=4
  DB指数推荐: K=4

======================================================================
聚类分析结果
======================================================================

各聚类样本数:
  聚类 0:  25 个样本 (25.00%)
  聚类 1:  28 个样本 (28.00%)
  聚类 2:  23 个样本 (23.00%)
  聚类 3:  24 个样本 (24.00%)

各聚类中心点(原始尺度):

聚类 0 中心特征:
  年龄         :   25.50
  年收入       :    8.20
  年消费       :    5.10
  购物频率     :    3.20
  在线时长     :    9.50

聚类 1 中心特征:
  年龄         :   54.50
  年收入       :   18.50
  年消费       :   10.80
  购物频率     :    2.80
  在线时长     :    3.50

...

聚类解释:
======================================================================

聚类 0: 低收入年轻群体 - 学生/刚毕业

聚类 1: 中收入中年群体 - 稳定职业

聚类 2: 高收入中年群体 - 企业高管

聚类 3: 高收入青年群体 - 创业者/高技能人才

聚类评估指标:
======================================================================
轮廓系数: 0.5845 (越接近1越好)
CH指数: 287.34 (越大越好)
DB指数: 0.5456 (越小越好)
惯性: 215.67
```

### 2. 可视化图表

#### kmeans_k_selection.png (K值选择四合一图)

**左上 - 肘部法则**:
- 惯性(Inertia)随K值变化的曲线
- 寻找"拐点"(肘部)
- 惯性下降速度明显减缓的位置

**右上 - 轮廓系数**:
- 衡量聚类质量的指标
- 范围[-1, 1],越接近1越好
- 星号标注最佳K值

**左下 - Calinski-Harabasz指数**:
- 类间离散度与类内离散度的比值
- 越大表示聚类越好
- 星号标注最佳K值

**右下 - Davies-Bouldin指数**:
- 衡量聚类重叠程度
- 越小表示聚类越好
- 星号标注最佳K值

#### kmeans_clusters_2d.png (2D聚类可视化)

使用PCA降维到2维展示聚类结果:
- 不同颜色代表不同聚类
- 黑色星号: 聚类中心
- 可以看出聚类的分离程度

#### kmeans_clusters_3d.png (3D聚类可视化)

使用PCA降维到3维展示聚类结果:
- 立体展示聚类分布
- 更直观的空间关系
- 可旋转查看(交互环境)

#### kmeans_feature_distributions.png (特征分布)

2×3子图展示5个特征在各聚类中的分布:
- 箱线图显示各聚类的统计特征
- 可以看出哪些特征最能区分聚类
- 识别每个聚类的典型特征

#### kmeans_principle.png (K-Means原理图)

**左图 - 聚类示意**:
- 3个聚类的数据点分布
- 中心点(黑色星号)
- 点到中心的连线(虚线)

**右图 - 算法流程**:
- 4个步骤的文字说明
- 清晰展示迭代过程

#### kmeans_radar_chart.png (聚类特征雷达图)

雷达图展示各聚类在不同特征上的表现:
- 每条线代表一个聚类
- 可以直观比较各聚类的特征特点
- 识别每个聚类的"画像"

## 🔑 核心概念详解

### 1. 聚类中心(质心)

每个聚类的中心点,是该聚类所有样本的均值:

```
质心 = (x₁ + x₂ + ... + xₙ) / n
```

- 代表聚类的"典型"样本
- 每次迭代都会更新
- 最终收敛到稳定位置

### 2. 惯性(Inertia)

所有样本到其所属聚类中心的距离平方和:

```
Inertia = Σ||xᵢ - μcᵢ||²
```

其中:
- xᵢ: 第i个样本
- μcᵢ: 第i个样本所属聚类的中心

**特点**:
- K越大,惯性越小
- K=n时,惯性=0(每个样本自己一类)
- 用于肘部法则选择K

### 3. 距离度量

K-Means默认使用**欧氏距离**:

```
d(x, y) = √[(y₁-x₁)² + (y₂-x₂)² + ... + (yₙ-xₙ)²]
```

**其他距离度量**:
- 曼哈顿距离: 适合稀疏数据
- 余弦相似度: 适合文本数据

### 4. K值选择

选择合适的K值是K-Means最重要的步骤。

#### 方法1: 肘部法则

观察惯性曲线,找到"拐点":
```
惯性
  |
  |      ╱
  |     ╱  ← 拐点(肘部)
  |    ╱
  |   ╱
  |  ╱
  | ╱
  |___________________ K
```

**优点**: 简单直观
**缺点**: 主观性强,拐点不明显时难判断

#### 方法2: 轮廓系数

衡量样本与自己聚类的相似度,以及与其他聚类的分离度:

```
s = (b - a) / max(a, b)
```

- a: 样本与同聚类其他样本的平均距离
- b: 样本与最近聚类样本的平均距离

**范围**:
- 接近1: 聚类效果好
- 接近0: 样本在聚类边界
- 接近-1: 样本可能分错了

#### 方法3: Calinski-Harabasz指数

类间离散度与类内离散度的比值:

```
CH = (BG / (k-1)) / (WG / (n-k))
```

- BG: Between-Group dispersion (类间离散度)
- WG: Within-Group dispersion (类内离散度)
- k: 聚类数
- n: 样本数

**越大越好**

#### 方法4: Davies-Bouldin指数

衡量聚类的平均相似度:

```
DB = (1/k) × Σ(maxᵢ≠ⱼ[(σᵢ + σⱼ) / d(cᵢ, cⱼ)])
```

**越小越好**

### 5. 特征标准化

**为什么必须标准化?**

不同特征的尺度不同会影响距离计算:

```
年龄: 20-60岁 (范围40)
年收入: 5-50万元 (范围45)

如果不标准化:
年龄差值: 10岁 → 距离贡献 ~100
年收入差值: 5万元 → 距离贡献 ~25
结果: 年龄对距离的影响是收入的4倍!
```

**解决方案**:
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # 均值0,方差1
```

## 💡 代码结构

```python
load_data()                      # 加载CSV数据
prepare_data()                   # 数据预处理和标准化
find_optimal_k()                 # 寻找最优K值
train_kmeans()                   # 训练K-Means模型
analyze_clusters()               # 分析聚类结果
visualize_k_selection()          # 可视化K值选择
visualize_clusters_2d()          # 2D聚类可视化
visualize_clusters_3d()          # 3D聚类可视化
visualize_feature_distributions() # 特征分布图
visualize_kmeans_principle()     # K-Means原理图
visualize_radar_chart()          # 雷达图
main()                           # 主函数
```

## 🎓 学习要点

1. **无监督学习**: 理解无标签的数据分析
2. **K值选择**: 掌握多种评估指标
3. **特征缩放**: 理解标准化的重要性
4. **聚类解释**: 学会给聚类赋予业务含义
5. **可视化**: 掌握高维数据的可视化方法

## 🔧 参数调优指南

### 选择最优K值

```python
# 方法1: 肘部法则
from sklearn.cluster import KMeans

inertias = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# 绘制曲线,找拐点
plt.plot(range(1, 11), inertias, 'o-')
```

```python
# 方法2: 轮廓系数
from sklearn.metrics import silhouette_score

scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    scores.append(score)

# 选择分数最高的K
best_k = scores.index(max(scores)) + 2
```

### 初始化方法

```python
# random: 随机选择(默认)
kmeans = KMeans(init='random', n_clusters=3)

# k-means++: 智能选择初始中心(推荐)
kmeans = KMeans(init='k-means++', n_clusters=3)
```

### 迭代参数

```python
# 最大迭代次数
kmeans = KMeans(max_iter=300)

# 收敛阈值
kmeans = KMeans(tol=1e-4)

# 初始化次数(选择最佳结果)
kmeans = KMeans(n_init=10)
```

## 📚 扩展应用

### 1. Mini-Batch K-Means

适合大数据集:

```python
from sklearn.cluster import MiniBatchKMeans

kmeans = MiniBatchKMeans(n_clusters=3, batch_size=100)
kmeans.fit(X)
```

### 2. K-Means++

改进的初始化方法:

```python
kmeans = KMeans(init='k-means++', n_clusters=3)
```

### 3. 图像压缩

使用K-Means进行颜色量化:

```python
from sklearn.cluster import KMeans
from skimage import io

# 读取图像
image = io.imread('image.jpg')
h, w, c = image.shape

# 将像素作为样本
pixels = image.reshape(-1, 3)

# 聚类
kmeans = KMeans(n_clusters=16)
kmeans.fit(pixels)

# 压缩图像
compressed = kmeans.cluster_centers_[kmeans.labels_]
compressed = compressed.reshape(h, w, c).astype(np.uint8)
```

### 4. 文本聚类

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# 文本向量化
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(documents)

# 聚类
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)
```

## ⚠️ 常见问题

**Q: K-Means和KNN有什么区别?**
A:
- KNN:监督学习,用K个邻居预测新样本
- K-Means:无监督学习,将数据分成K个聚类

**Q: 如何选择K值?**
A: 综合使用多种方法:
- 肘部法则(惯性曲线)
- 轮廓系数(越接近1越好)
- 业务知识(期望分几类)

**Q: K-Means必须标准化特征吗?**
A: **必须!** 不同尺度会严重影响距离计算。

**Q: K-Means能处理分类特征吗?**
A: 不能直接处理。需要:
- 编码为数值(独热编码)
- 使用K-Modes算法
- 使用K-Prototypes算法(混合特征)

**Q: K-Means对异常值敏感吗?**
A: 敏感。异常值会严重影响质心计算。建议:
- 预处理去除异常值
- 使用K-Medoids算法(对异常值鲁棒)

**Q: K-Means会收敛吗?**
A: 会。K-Means保证收敛,但可能收敛到局部最优。

**Q: 如何避免局部最优?**
A:
- 多次运行,选择最佳结果
- 使用k-means++初始化
- 增加n_init参数

**Q: K-Means的时间复杂度是多少?**
A: O(t×k×n×d)
- t: 迭代次数
- k: 聚类数
- n: 样本数
- d: 特征数

## 📊 算法复杂度对比

| 指标 | K-Means | 层次聚类 | DBSCAN |
|------|---------|----------|--------|
| **时间复杂度** | O(tkn) | O(n²) | O(n log n) |
| **空间复杂度** | O(n+k) | O(n²) | O(n) |
| **需要指定K** | 是 | 否 | 否 |
| **发现任意形状** | 否 | 是 | 是 |
| **处理异常值** | 差 | 中 | 好 |
| **适用数据规模** | 大 | 小 | 大 |

## 🎯 实践建议

1. **数据预处理**: 必须标准化特征
2. **K值选择**: 使用多种评估指标综合判断
3. **多次运行**: 避免局部最优,n_init=10+
4. **异常值处理**: 去除或使用鲁棒算法
5. **特征选择**: 移除无关特征
6. **可视化**: 使用PCA/t-SNE降维可视化
7. **业务解释**: 结合业务知识解释聚类

## 📝 K-Means优缺点总结

### 优点 ✅
- 简单易实现
- 时间复杂度相对较低
- 可解释性强
- 适合大数据集
- 广泛应用

### 缺点 ❌
- 需要预先指定K值
- 对初始值敏感
- 只能发现球形聚类
- 对异常值敏感
- 不适合非凸形状
- 只能处理数值特征

## 🔄 与其他聚类算法对比

| 算法 | 类型 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **K-Means** | 划分式 | 快速、可扩展 | 需指定K、球形聚类 | 大数据、球形聚类 |
| **层次聚类** | 层次式 | 不需指定K、树状图 | 慢、不适合大数据 | 小数据、需要层次结构 |
| **DBSCAN** | 密度式 | 发现任意形状、处理噪声 | 难以处理不同密度 | 空间数据、有噪声 |
| **GMM** | 概率式 | 软聚类、概率输出 | 复杂、易过拟合 | 需要概率、软聚类 |

## 📊 现在拥有的完整算法库

| 算法 | 类型 | 有监督/无监督 | 文件 |
|------|------|--------------|------|
| **线性回归** | 回归 | 有监督 | linear_regression.py |
| **逻辑回归** | 分类 | 有监督 | logistic_regression_multi.py |
| **SVM** | 分类 | 有监督 | svm_example.py |
| **KNN** | 分类 | 有监督 | knn_example.py |
| **K-Means** | 聚类 | 无监督 | kmeans_example.py |

恭喜你!现在你已经掌握了监督学习和无监督学习的核心算法! 🎉
