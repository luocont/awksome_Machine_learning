"""
é€»è¾‘å›å½’ - å¤šç‰¹å¾éšæœºæ•°æ®ç‰ˆæœ¬
ä½¿ç”¨å¤šä¸ªå­¦ä¹ æŒ‡æ ‡é¢„æµ‹å­¦ç”Ÿè€ƒè¯•æ˜¯å¦é€šè¿‡
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
import os

# è®¾ç½® matplotlib ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def load_data(csv_path):
    """åŠ è½½CSVæ•°æ®"""
    data = pd.read_csv(csv_path, encoding='utf-8')
    print("=" * 70)
    print("æ•°æ®é¢„è§ˆ:")
    print(data.head(10))
    print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    print(data.describe())
    print("\né€šè¿‡æƒ…å†µç»Ÿè®¡:")
    print(data['é€šè¿‡çŠ¶æ€'].value_counts())
    print(f"é€šè¿‡ç‡: {data['é€šè¿‡çŠ¶æ€'].mean() * 100:.2f}%")
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    return data

def prepare_data(data):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    feature_columns = ['å­¦ä¹ æ—¶é—´(å°æ—¶)', 'å‡ºå‹¤ç‡(%)', 'ä½œä¸šå®Œæˆç‡(%)', 'è€ƒå‰å¤ä¹ (å°æ—¶)']
    X = data[feature_columns].values
    y = data['é€šè¿‡çŠ¶æ€'].values
    return X, y, feature_columns

def train_model(X_train, y_train):
    """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    model = LogisticRegression(random_state=42, max_iter=1000)
    model.fit(X_train_scaled, y_train)
    return model, scaler

def evaluate_model(model, scaler, X_test, y_test):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
    X_test_scaled = scaler.transform(X_test)

    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    # è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred,
                                   target_names=['ä¸é€šè¿‡', 'é€šè¿‡'])

    # ROCæ›²çº¿
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    return {
        'accuracy': accuracy,
        'conf_matrix': conf_matrix,
        'report': report,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba,
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc
    }

def visualize_results(metrics):
    """å¯è§†åŒ–ç»“æœ"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # å·¦å›¾: æ··æ·†çŸ©é˜µ
    ax1 = axes[0]
    conf_matrix = metrics['conf_matrix']
    im = ax1.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
    ax1.figure.colorbar(im, ax=ax1)

    thresh = conf_matrix.max() / 2
    for i in range(conf_matrix.shape[0]):
        for j in range(conf_matrix.shape[1]):
            ax1.text(j, i, format(conf_matrix[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if conf_matrix[i, j] > thresh else "black",
                    fontsize=18)

    ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=13)
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=13)
    ax1.set_title(f'æ··æ·†çŸ©é˜µ\n(å‡†ç¡®ç‡: {metrics["accuracy"]*100:.2f}%)', fontsize=14)
    ax1.set_xticks([0, 1])
    ax1.set_yticks([0, 1])
    ax1.set_xticklabels(['ä¸é€šè¿‡', 'é€šè¿‡'])
    ax1.set_yticklabels(['ä¸é€šè¿‡', 'é€šè¿‡'])

    # å³å›¾: ROCæ›²çº¿
    ax2 = axes[1]
    ax2.plot(metrics['fpr'], metrics['tpr'], color='blue',
             linewidth=2.5, label=f'ROCæ›²çº¿ (AUC = {metrics["roc_auc"]:.4f})')
    ax2.plot([0, 1], [0, 1], color='red', linestyle='--',
             linewidth=2, label='éšæœºåˆ†ç±»å™¨')

    # æ·»åŠ æœ€ä½³ç‚¹æ ‡æ³¨
    optimal_idx = np.argmax(metrics['tpr'] - metrics['fpr'])
    optimal_threshold = metrics['fpr'][optimal_idx]
    ax2.plot(metrics['fpr'][optimal_idx], metrics['tpr'][optimal_idx], 'go',
             markersize=10, label='æœ€ä½³é˜ˆå€¼ç‚¹')

    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å‡æ­£ç‡ (False Positive Rate)', fontsize=12)
    ax2.set_ylabel('çœŸæ­£ç‡ (True Positive Rate)', fontsize=12)
    ax2.set_title('ROC æ›²çº¿', fontsize=14)
    ax2.legend(loc="lower right", fontsize=11)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('logistic_regression_multi_result.png', dpi=300, bbox_inches='tight')
    print("\nå¯è§†åŒ–ç»“æœå·²ä¿å­˜ä¸º 'logistic_regression_multi_result.png'")

def visualize_feature_importance(model, feature_columns):
    """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
    fig, ax = plt.subplots(figsize=(10, 6))

    # è·å–ç³»æ•°ç»å¯¹å€¼
    importance = np.abs(model.coef_[0])
    indices = np.argsort(importance)

    # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
    bars = ax.barh(range(len(importance)), importance[indices], color=colors)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, idx) in enumerate(zip(bars, indices)):
        width = bar.get_width()
        ax.text(width, bar.get_y() + bar.get_height()/2,
               f'{importance[idx]:.3f}',
               ha='left', va='center', fontsize=11, fontweight='bold')

    ax.set_yticks(range(len(importance)))
    ax.set_yticklabels([feature_columns[i] for i in indices])
    ax.set_xlabel('ç‰¹å¾é‡è¦æ€§ (ç³»æ•°ç»å¯¹å€¼)', fontsize=13)
    ax.set_title('é€»è¾‘å›å½’ - ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=15)
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    print("ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜ä¸º 'feature_importance.png'")

def visualize_data_distribution(data, feature_columns):
    """å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    colors_pass = '#2ecc71'
    colors_fail = '#e74c3c'

    for idx, feature in enumerate(feature_columns):
        ax = axes[idx]

        # åˆ†åˆ«ç»˜åˆ¶é€šè¿‡å’Œæœªé€šè¿‡çš„æ•°æ®
        pass_data = data[data['é€šè¿‡çŠ¶æ€'] == 1][feature]
        fail_data = data[data['é€šè¿‡çŠ¶æ€'] == 0][feature]

        ax.hist(fail_data, bins=15, color=colors_fail, alpha=0.6,
               label='ä¸é€šè¿‡', edgecolor='black')
        ax.hist(pass_data, bins=15, color=colors_pass, alpha=0.6,
               label='é€šè¿‡', edgecolor='black')

        ax.set_xlabel(feature, fontsize=11)
        ax.set_ylabel('æ ·æœ¬æ•°', fontsize=11)
        ax.set_title(f'{feature} åˆ†å¸ƒ', fontsize=13)
        ax.legend(fontsize=10)
        ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig('data_distribution.png', dpi=300, bbox_inches='tight')
    print("æ•°æ®åˆ†å¸ƒå›¾å·²ä¿å­˜ä¸º 'data_distribution.png'")

def create_animation(data, feature_columns):
    """åˆ›å»ºé€»è¾‘å›å½’åŠ¨ç”» - é€æ­¥æ·»åŠ æ•°æ®ç‚¹"""
    print("\n" + "=" * 70)
    print("å¼€å§‹ç”Ÿæˆé€»è¾‘å›å½’åŠ¨ç”»...")
    print("=" * 70)

    # åˆ›å»ºä¿å­˜å¸§çš„ç›®å½•
    frames_dir = 'animation_frames'
    if not os.path.exists(frames_dir):
        os.makedirs(frames_dir)

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾(å­¦ä¹ æ—¶é—´)è¿›è¡Œ2Då¯è§†åŒ–
    feature_name = feature_columns[0]
    X_single = data[feature_name].values.reshape(-1, 1)
    y = data['é€šè¿‡çŠ¶æ€'].values

    # æ‰“ä¹±æ•°æ®é¡ºåºï¼Œä½¿åŠ¨ç”»æ›´æœ‰è¶£
    # ç¡®ä¿æ‰“ä¹±åçš„æ•°æ®å‰å‡ ä¸ªç‚¹åŒ…å«ä¸¤ä¸ªç±»åˆ«
    pass_indices = np.where(y == 1)[0]
    fail_indices = np.where(y == 0)[0]

    np.random.shuffle(pass_indices)
    np.random.shuffle(fail_indices)

    # äº¤æ›¿æ’åˆ—ä¸¤ç±»æ ·æœ¬ï¼Œç¡®ä¿æ—©æœŸå¸§æœ‰ä¸¤ä¸ªç±»åˆ«
    indices = []
    min_len = min(len(pass_indices), len(fail_indices))
    for i in range(min_len):
        indices.append(pass_indices[i])
        indices.append(fail_indices[i])

    # æ·»åŠ å‰©ä½™çš„æ ·æœ¬
    remaining = pass_indices[min_len:] if len(pass_indices) > min_len else fail_indices[min_len:]
    indices.extend(remaining)

    indices = np.array(indices)
    X_shuffled = X_single[indices]
    y_shuffled = y[indices]

    print(f"æ€»å…±æœ‰ {len(X_single)} ä¸ªæ•°æ®ç‚¹")
    print(f"ä½¿ç”¨ç‰¹å¾: {feature_name}")

    # ä¸ºæ¯ä¸ªæ•°æ®ç‚¹æ•°é‡ç”Ÿæˆä¸€å¸§(è‡³å°‘éœ€è¦2ä¸ªç‚¹æ‰èƒ½æ‹Ÿåˆ)
    for n_points in range(3, len(X_shuffled) + 1):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # å½“å‰æ•°æ®ç‚¹
        x_current = X_shuffled[:n_points].flatten()
        y_current = y_shuffled[:n_points]

        # æ ‡å‡†åŒ–æ•°æ®
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        x_current_scaled = scaler.fit_transform(x_current.reshape(-1, 1))

        # è®­ç»ƒé€»è¾‘å›å½’
        model = LogisticRegression(random_state=42, max_iter=1000)
        model.fit(x_current_scaled, y_current)

        # è®¡ç®—å‡†ç¡®ç‡
        y_pred = model.predict(x_current_scaled)
        accuracy = accuracy_score(y_current, y_pred)

        # ç”Ÿæˆå¹³æ»‘çš„æ›²çº¿
        x_range = np.linspace(X_single.min() - 0.5, X_single.max() + 0.5, 300).reshape(-1, 1)
        x_range_scaled = scaler.transform(x_range)
        y_proba = model.predict_proba(x_range_scaled)[:, 1]

        # å·¦å›¾: æ•°æ®ç‚¹å’Œå†³ç­–è¾¹ç•Œ
        ax1.scatter(X_shuffled[n_points:].flatten(), y_shuffled[n_points:],
                   c='lightgray', s=80, alpha=0.3, edgecolors='gray',
                   label='å¾…æ·»åŠ ç‚¹', zorder=1)

        # ç»˜åˆ¶å½“å‰å·²æœ‰çš„ç‚¹
        pass_mask = y_current[:-1] == 1
        fail_mask = y_current[:-1] == 0

        ax1.scatter(x_current[:-1][fail_mask], y_current[:-1][fail_mask],
                   c='#e74c3c', s=100, alpha=0.7, edgecolors='black',
                   linewidth=1, label='ä¸é€šè¿‡', zorder=3)
        ax1.scatter(x_current[:-1][pass_mask], y_current[:-1][pass_mask],
                   c='#2ecc71', s=100, alpha=0.7, edgecolors='black',
                   linewidth=1, label='é€šè¿‡', zorder=3)

        # æœ€æ–°æ·»åŠ çš„ç‚¹é«˜äº®æ˜¾ç¤º
        last_color = '#2ecc71' if y_current[-1] == 1 else '#e74c3c'
        ax1.scatter([x_current[-1]], [y_current[-1]], s=350, c=last_color,
                   alpha=0.9, edgecolors='yellow', linewidth=3,
                   zorder=4, label='æ–°å¢ç‚¹')

        # ç»˜åˆ¶æ¦‚ç‡æ›²çº¿
        ax1.plot(x_range, y_proba, 'b-', linewidth=3, label='é¢„æµ‹æ¦‚ç‡', zorder=2)
        ax1.axhline(y=0.5, color='orange', linestyle='--', linewidth=2,
                   label='å†³ç­–é˜ˆå€¼', alpha=0.7, zorder=1)

        # å†³ç­–è¾¹ç•Œ
        decision_boundary = x_range[np.argmin(np.abs(y_proba - 0.5))]
        ax1.axvline(x=decision_boundary, color='purple', linestyle=':',
                   linewidth=2, alpha=0.7, label='å†³ç­–è¾¹ç•Œ', zorder=1)

        # æ·»åŠ ä¿¡æ¯æ¡†
        info_text = f'æ•°æ®ç‚¹æ•°: {n_points}/{len(X_single)}\n'
        info_text += f'å‡†ç¡®ç‡: {accuracy*100:.2f}%\n'
        info_text += f'é€šè¿‡æ ·æœ¬: {y_current.sum()}\n'
        info_text += f'ä¸é€šè¿‡æ ·æœ¬: {(1-y_current).sum()}'

        ax1.text(0.02, 0.98, info_text, transform=ax1.transAxes, fontsize=11,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))

        ax1.set_xlim(X_single.min() - 0.5, X_single.max() + 0.5)
        ax1.set_ylim(-0.1, 1.1)
        ax1.set_xlabel(feature_name, fontsize=13)
        ax1.set_ylabel('é€šè¿‡æ¦‚ç‡', fontsize=13)
        ax1.set_title(f'é€»è¾‘å›å½’åŠ¨æ€æ¼”ç¤º - ç¬¬ {n_points} ä¸ªæ•°æ®ç‚¹',
                     fontsize=14, fontweight='bold')
        ax1.legend(loc='center right', fontsize=9)
        ax1.grid(True, alpha=0.3)

        # å³å›¾: Sigmoidå‡½æ•°å¯è§†åŒ–
        z = x_range_scaled.flatten()
        sigmoid = 1 / (1 + np.exp(-z))

        ax2.plot(z, sigmoid, 'b-', linewidth=3, label='Sigmoidå‡½æ•°')
        ax2.axhline(y=0.5, color='orange', linestyle='--', linewidth=2,
                   label='é˜ˆå€¼ (0.5)', alpha=0.7)
        ax2.axvline(x=0, color='purple', linestyle=':', linewidth=2,
                   label='å†³ç­–è¾¹ç•Œ (z=0)', alpha=0.7)

        # æ ‡è®°å½“å‰æ•°æ®ç‚¹åœ¨Sigmoidä¸Šçš„ä½ç½®
        current_z = x_current_scaled.flatten()
        current_sigmoid = 1 / (1 + np.exp(-current_z))

        # æ ¹æ®ç±»åˆ«ç€è‰²
        fail_z = current_z[y_current == 0]
        fail_sigmoid = current_sigmoid[y_current == 0]
        pass_z = current_z[y_current == 1]
        pass_sigmoid = current_sigmoid[y_current == 1]

        ax2.scatter(fail_z, fail_sigmoid, c='#e74c3c', s=80, alpha=0.6,
                   edgecolors='black', linewidth=1, label='ä¸é€šè¿‡', zorder=3)
        ax2.scatter(pass_z, pass_sigmoid, c='#2ecc71', s=80, alpha=0.6,
                   edgecolors='black', linewidth=1, label='é€šè¿‡', zorder=3)

        # æœ€æ–°æ·»åŠ çš„ç‚¹é«˜äº®
        last_z = current_z[-1]
        last_sigmoid = current_sigmoid[-1]
        last_color = '#2ecc71' if y_current[-1] == 1 else '#e74c3c'
        ax2.scatter([last_z], [last_sigmoid], s=250, c=last_color,
                   alpha=0.9, edgecolors='yellow', linewidth=3, zorder=4)

        # Sigmoidå‡½æ•°ä¿¡æ¯
        sigmoid_info = f'æ¨¡å‹å‚æ•°:\n'
        sigmoid_info += f'ç³»æ•°: {model.coef_[0][0]:.4f}\n'
        sigmoid_info += f'æˆªè·: {model.intercept_[0]:.4f}\n\n'
        sigmoid_info += f'Sigmoidå…¬å¼:\n'
        sigmoid_info += f'P(y=1) = 1/(1+e^-z)\n'
        sigmoid_info += f'z = {model.coef_[0][0]:.2f}Ã—x + {model.intercept_[0]:.2f}'

        ax2.text(0.02, 0.97, sigmoid_info, transform=ax2.transAxes, fontsize=10,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9))

        ax2.set_xlim(z.min() - 0.5, z.max() + 0.5)
        ax2.set_ylim(-0.1, 1.1)
        ax2.set_xlabel('æ ‡å‡†åŒ–ç‰¹å¾å€¼ (z)', fontsize=12)
        ax2.set_ylabel('æ¦‚ç‡ P(y=1)', fontsize=12)
        ax2.set_title('Sigmoidå˜æ¢', fontsize=14, fontweight='bold')
        ax2.legend(loc='lower right', fontsize=9)
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜è¿™ä¸€å¸§
        frame_filename = os.path.join(frames_dir, f'frame_{n_points:03d}.png')
        plt.savefig(frame_filename, dpi=100, bbox_inches='tight')
        plt.close(fig)

        if n_points % 5 == 0 or n_points == len(X_shuffled):
            print(f"  å·²ç”Ÿæˆ {n_points}/{len(X_shuffled)} å¸§")

    print(f"\næ‰€æœ‰å¸§å·²ä¿å­˜åˆ°: {frames_dir}/")

    # å°è¯•ç”ŸæˆGIF
    try:
        from PIL import Image
        print("\næ­£åœ¨ç”ŸæˆGIFåŠ¨ç”»...")

        frames = []
        for n_points in range(3, len(X_shuffled) + 1):
            frame_filename = os.path.join(frames_dir, f'frame_{n_points:03d}.png')
            img = Image.open(frame_filename)
            frames.append(img)

        # ä¿å­˜ä¸ºGIF
        gif_path = 'logistic_regression_animation.gif'
        frames[0].save(gif_path,
                       save_all=True,
                       append_images=frames[1:],
                       duration=600,  # æ¯å¸§600æ¯«ç§’
                       loop=0)  # æ— é™å¾ªç¯

        print(f"âœ… GIFåŠ¨ç”»å·²ä¿å­˜ä¸º: {gif_path}")

    except ImportError:
        print("âš ï¸  PILæœªå®‰è£…ï¼Œæ— æ³•ç”ŸæˆGIF")
        print("   å®‰è£…æ–¹æ³•: pip install Pillow")

    print("\nğŸ“Š åŠ¨ç”»è¯´æ˜:")
    print("- ç»¿è‰²ç‚¹: é€šè¿‡çš„å­¦ç”Ÿ")
    print("- çº¢è‰²ç‚¹: ä¸é€šè¿‡çš„å­¦ç”Ÿ")
    print("- é»„è‰²å…‰åœˆ: æœ€æ–°æ·»åŠ çš„ç‚¹")
    print("- ç°è‰²ç‚¹: å¾…æ·»åŠ çš„æ•°æ®ç‚¹")
    print("- è“è‰²æ›²çº¿: å­¦ä¹ åˆ°çš„æ¦‚ç‡é¢„æµ‹æ›²çº¿")
    print("- å·¦å›¾: åŸå§‹ç‰¹å¾ç©ºé—´çš„å†³ç­–è¾¹ç•Œ")
    print("- å³å›¾: Sigmoidå‡½æ•°å˜æ¢åçš„æ¦‚ç‡ç©ºé—´")
    print("- è§‚å¯Ÿå†³ç­–è¾¹ç•Œå’ŒSigmoidæ›²çº¿å¦‚ä½•éšç€æ•°æ®å¢åŠ è€Œå˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("é€»è¾‘å›å½’ç¤ºä¾‹ - å¤šç‰¹å¾å­¦ç”Ÿè€ƒè¯•é€šè¿‡é¢„æµ‹")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    csv_path = 'logistic_regression_sample.csv'
    data = load_data(csv_path)

    # 2. å‡†å¤‡æ•°æ®
    X, y, feature_columns = prepare_data(data)

    # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    print(f"ç‰¹å¾æ•°é‡: {X_train.shape[1]}")

    # 4. è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model, scaler = train_model(X_train, y_train)
    print("æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    # 5. æ¨¡å‹å‚æ•°
    print(f"\n{'='*70}")
    print("æ¨¡å‹å‚æ•°åˆ†æ:")
    print(f"{'='*70}")
    for feature, coef in zip(feature_columns, model.coef_[0]):
        print(f"{feature:20s} : ç³»æ•° = {coef:8.4f}")
    print(f"{'æˆªè·':20s} : {model.intercept_[0]:8.4f}")

    # 6. è¯„ä¼°æ¨¡å‹
    print(f"\n{'='*70}")
    print("æ¨¡å‹è¯„ä¼°:")
    print(f"{'='*70}")
    metrics = evaluate_model(model, scaler, X_test, y_test)
    print(f"å‡†ç¡®ç‡: {metrics['accuracy'] * 100:.2f}%")
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(metrics['conf_matrix'])
    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(metrics['report'])
    print(f"\nAUCå€¼: {metrics['roc_auc']:.4f}")

    # 7. ç¤ºä¾‹é¢„æµ‹
    print(f"\n{'='*70}")
    print("ç¤ºä¾‹é¢„æµ‹:")
    print(f"{'='*70}")

    sample_students = np.array([
        [3.0, 70, 55, 1.0],   # å­¦ä¹ æ—¶é—´çŸ­
        [6.0, 85, 78, 2.5],   # ä¸­ç­‰æ°´å¹³
        [9.0, 98, 98, 4.0]    # å­¦ä¹ æ—¶é—´é•¿
    ])

    # æ ‡å‡†åŒ–
    sample_scaled = scaler.transform(sample_students)
    sample_predictions = model.predict(sample_scaled)
    sample_probabilities = model.predict_proba(sample_scaled)

    descriptions = [
        "å­¦ä¹ æ—¶é—´çŸ­, å‡ºå‹¤ç‡ä½",
        "ä¸­ç­‰å­¦ä¹ æ°´å¹³",
        "å­¦ä¹ è®¤çœŸ, å‡†å¤‡å……åˆ†"
    ]

    for i, (desc, pred, prob) in enumerate(zip(descriptions,
                                               sample_predictions,
                                               sample_probabilities), 1):
        status = "âœ“ é€šè¿‡" if pred == 1 else "âœ— ä¸é€šè¿‡"
        print(f"\nå­¦ç”Ÿ{i} ({desc}):")
        print(f"  å­¦ä¹ æ—¶é—´: {sample_students[i-1][0]}å°æ—¶")
        print(f"  å‡ºå‹¤ç‡: {sample_students[i-1][1]}%")
        print(f"  ä½œä¸šå®Œæˆç‡: {sample_students[i-1][2]}%")
        print(f"  è€ƒå‰å¤ä¹ : {sample_students[i-1][3]}å°æ—¶")
        print(f"  é¢„æµ‹ç»“æœ: {status}")
        print(f"  é€šè¿‡æ¦‚ç‡: {prob[1]*100:.2f}%")

    # 8. å¯è§†åŒ–
    print(f"\n{'='*70}")
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print(f"{'='*70}")
    visualize_results(metrics)
    visualize_feature_importance(model, feature_columns)
    visualize_data_distribution(data, feature_columns)

    # 9. ç”ŸæˆåŠ¨ç”»
    create_animation(data, feature_columns)

    print(f"\n{'='*70}")
    print("é€»è¾‘å›å½’åˆ†æå®Œæˆ!")
    print("\nç”Ÿæˆæ–‡ä»¶:")
    print("  - logistic_regression_multi_result.png (æ··æ·†çŸ©é˜µ+ROCæ›²çº¿)")
    print("  - feature_importance.png (ç‰¹å¾é‡è¦æ€§)")
    print("  - data_distribution.png (æ•°æ®åˆ†å¸ƒ)")
    print("  - logistic_regression_animation.gif (å­¦ä¹ è¿‡ç¨‹åŠ¨ç”»)")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()
