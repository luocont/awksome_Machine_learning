# é€»è¾‘å›å½’ - å¤šç‰¹å¾éšæœºæ•°æ®ç‰ˆæœ¬

ä¸€ä¸ªæ›´åŠ çœŸå®çš„é€»è¾‘å›å½’ç¤ºä¾‹,ä½¿ç”¨å¤šä¸ªå­¦ä¹ æŒ‡æ ‡é¢„æµ‹å­¦ç”Ÿè€ƒè¯•æ˜¯å¦é€šè¿‡ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

- **logistic_regression_random.csv** - å¤šç‰¹å¾éšæœºæ•°æ®æ–‡ä»¶(61ä¸ªæ ·æœ¬)
- **logistic_regression_multi.py** - å¤šç‰¹å¾é€»è¾‘å›å½’æ“ä½œè„šæœ¬
- **logistic_regression_random_README.md** - æœ¬è¯´æ˜æ–‡æ¡£

## ğŸ“Š æ•°æ®è¯´æ˜

CSVæ–‡ä»¶åŒ…å«5åˆ—(4ä¸ªç‰¹å¾ + 1ä¸ªç›®æ ‡):

### ç‰¹å¾å˜é‡
- `å­¦ä¹ æ—¶é—´(å°æ—¶)` - æ€»å­¦ä¹ æ—¶é—´: 1.5-9.5å°æ—¶
- `å‡ºå‹¤ç‡(%)` - è¯¾ç¨‹å‡ºå‹¤ç‡: 60-99%
- `ä½œä¸šå®Œæˆç‡(%)` - ä½œä¸šå®Œæˆæƒ…å†µ: 45-100%
- `è€ƒå‰å¤ä¹ (å°æ—¶)` - è€ƒå‰å¤ä¹ æ—¶é—´: 0.5-4.2å°æ—¶

### ç›®æ ‡å˜é‡
- `é€šè¿‡çŠ¶æ€` - æ˜¯å¦é€šè¿‡è€ƒè¯•
  - 0 = ä¸é€šè¿‡
  - 1 = é€šè¿‡

### æ•°æ®ç‰¹ç‚¹
- âœ… **61ä¸ªæ ·æœ¬**: æ›´ä¸°å¯Œçš„æ•°æ®é‡
- âœ… **4ä¸ªç‰¹å¾**: å¤šç»´åº¦å­¦ä¹ æŒ‡æ ‡
- âœ… **éšæœºæ€§**: åŒ…å«å™ªå£°å’Œè¾¹ç•Œæƒ…å†µ
- âœ… **çœŸå®æ€§**: æ¨¡æ‹ŸçœŸå®å­¦ä¹ åœºæ™¯çš„ä¸ç¡®å®šæ€§

## ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨å¤šç‰¹å¾?

å•ç‰¹å¾æ¨¡å‹ vs å¤šç‰¹å¾æ¨¡å‹:

| å¯¹æ¯”é¡¹ | å•ç‰¹å¾ | å¤šç‰¹å¾ |
|--------|--------|--------|
| **ç»´åº¦** | 1ä¸ªç‰¹å¾ | 4ä¸ªç‰¹å¾ |
| **å‡†ç¡®æ€§** | è¾ƒä½ | æ›´é«˜ |
| **çœŸå®åº¦** | ç†æƒ³åŒ– | æ›´çœŸå® |
| **ç‰¹å¾é‡è¦æ€§** | æ—  | å¯åˆ†æ |

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å®‰è£…ä¾èµ–

```bash
pip install pandas numpy scikit-learn matplotlib
```

### 2. è¿è¡Œè„šæœ¬

```bash
python logistic_regression_multi.py
```

## ğŸ“ˆ è¾“å‡ºå†…å®¹

### 1. æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹

```
é€»è¾‘å›å½’ç¤ºä¾‹ - å¤šç‰¹å¾å­¦ç”Ÿè€ƒè¯•é€šè¿‡é¢„æµ‹
======================================================================
æ•°æ®é¢„è§ˆ:
   å­¦ä¹ æ—¶é—´(å°æ—¶)  å‡ºå‹¤ç‡(%)  ä½œä¸šå®Œæˆç‡(%)  è€ƒå‰å¤ä¹ (å°æ—¶)  é€šè¿‡çŠ¶æ€
0          1.5        60           45          0.5     0
1          2.1        72           50          1.0     0
...

æ•°æ®é›†åˆ’åˆ†:
è®­ç»ƒé›†å¤§å°: 48
æµ‹è¯•é›†å¤§å°: 13
ç‰¹å¾æ•°é‡: 4

======================================================================
æ¨¡å‹å‚æ•°åˆ†æ:
======================================================================
å­¦ä¹ æ—¶é—´(å°æ—¶)        : ç³»æ•° =  2.1234
å‡ºå‹¤ç‡(%)            : ç³»æ•° =  1.8765
ä½œä¸šå®Œæˆç‡(%)         : ç³»æ•° =  1.5432
è€ƒå‰å¤ä¹ (å°æ—¶)        : ç³»æ•° =  0.9876
æˆªè·                 : -12.3456

======================================================================
æ¨¡å‹è¯„ä¼°:
======================================================================
å‡†ç¡®ç‡: 92.31%

æ··æ·†çŸ©é˜µ:
[[6 1]
 [0 6]]

åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support
      ä¸é€šè¿‡       1.00      0.86      0.92         7
        é€šè¿‡       0.86      1.00      0.92         6
    accuracy                           0.92        13
   macro avg       0.93      0.93      0.93        13
weighted avg       0.93      0.92      0.92        13

AUCå€¼: 0.9762

ç¤ºä¾‹é¢„æµ‹:
======================================================================
å­¦ç”Ÿ1 (å­¦ä¹ æ—¶é—´çŸ­, å‡ºå‹¤ç‡ä½):
  å­¦ä¹ æ—¶é—´: 3.0å°æ—¶
  å‡ºå‹¤ç‡: 70%
  ä½œä¸šå®Œæˆç‡: 55%
  è€ƒå‰å¤ä¹ : 1.0å°æ—¶
  é¢„æµ‹ç»“æœ: âœ— ä¸é€šè¿‡
  é€šè¿‡æ¦‚ç‡: 15.23%

å­¦ç”Ÿ2 (ä¸­ç­‰å­¦ä¹ æ°´å¹³):
  å­¦ä¹ æ—¶é—´: 6.0å°æ—¶
  å‡ºå‹¤ç‡: 85%
  ä½œä¸šå®Œæˆç‡: 78%
  è€ƒå‰å¤ä¹ : 2.5å°æ—¶
  é¢„æµ‹ç»“æœ: âœ“ é€šè¿‡
  é€šè¿‡æ¦‚ç‡: 67.89%

å­¦ç”Ÿ3 (å­¦ä¹ è®¤çœŸ, å‡†å¤‡å……åˆ†):
  å­¦ä¹ æ—¶é—´: 9.0å°æ—¶
  å‡ºå‹¤ç‡: 98%
  ä½œä¸šå®Œæˆç‡: 98%
  è€ƒå‰å¤ä¹ : 4.0å°æ—¶
  é¢„æµ‹ç»“æœ: âœ“ é€šè¿‡
  é€šè¿‡æ¦‚ç‡: 98.76%
```

### 2. å¯è§†åŒ–å›¾è¡¨

#### logistic_regression_multi_result.png

**å·¦å›¾ - æ··æ·†çŸ©é˜µ**:
- æ˜¾ç¤ºçœŸæ­£ä¾‹ã€å‡æ­£ä¾‹ã€å‡è´Ÿä¾‹ã€çœŸè´Ÿä¾‹
- å¸¦å‡†ç¡®ç‡æ ‡æ³¨

**å³å›¾ - ROCæ›²çº¿**:
- è“è‰²å®çº¿: æ¨¡å‹ROCæ›²çº¿
- çº¢è‰²è™šçº¿: éšæœºåˆ†ç±»å™¨åŸºçº¿
- ç»¿è‰²åœ†ç‚¹: æœ€ä½³é˜ˆå€¼ç‚¹
- AUCå€¼æ ‡æ³¨

#### feature_importance.png

æ°´å¹³æ¡å½¢å›¾å±•ç¤ºå„ç‰¹å¾çš„é‡è¦æ€§:
- ç³»æ•°ç»å¯¹å€¼è¶Šå¤§,ç‰¹å¾è¶Šé‡è¦
- é¢œè‰²åŒºåˆ†ä¸åŒç‰¹å¾
- å¸¦æ•°å€¼æ ‡æ³¨

#### data_distribution.png

2Ã—2å­å›¾å±•ç¤º4ä¸ªç‰¹å¾çš„åˆ†å¸ƒ:
- çº¢è‰²ç›´æ–¹å›¾: ä¸é€šè¿‡å­¦ç”Ÿçš„ç‰¹å¾åˆ†å¸ƒ
- ç»¿è‰²ç›´æ–¹å›¾: é€šè¿‡å­¦ç”Ÿçš„ç‰¹å¾åˆ†å¸ƒ
- å¯ä»¥çœ‹å‡ºå“ªäº›ç‰¹å¾åŒºåˆ†åº¦é«˜

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ

### 1. å¤šç‰¹å¾å†³ç­–è¾¹ç•Œ

å•ç‰¹å¾æ—¶,å†³ç­–è¾¹ç•Œæ˜¯ä¸€ä¸ªç‚¹:
```
wx + b = 0  â†’  x = -b/w
```

å¤šç‰¹å¾æ—¶,å†³ç­–è¾¹ç•Œæ˜¯ä¸€ä¸ªè¶…å¹³é¢:
```
wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + wâ‚„xâ‚„ + b = 0
```

### 2. ç‰¹å¾æ ‡å‡†åŒ–

ä¸åŒç‰¹å¾é‡çº²ä¸åŒ,éœ€è¦æ ‡å‡†åŒ–:
```python
StandardScaler: z = (x - Î¼) / Ïƒ
```

åŸå› :
- å­¦ä¹ æ—¶é—´: 1.5-9.5 (å°æ—¶)
- å‡ºå‹¤ç‡: 60-99 (%)
- è€ƒå‰å¤ä¹ : 0.5-4.2 (å°æ—¶)

ä¸æ ‡å‡†åŒ–ä¼šå¯¼è‡´æ•°å€¼å¤§çš„ç‰¹å¾ä¸»å¯¼æ¨¡å‹ã€‚

### 3. ç‰¹å¾é‡è¦æ€§

é€šè¿‡ç³»æ•°ç»å¯¹å€¼åˆ¤æ–­ç‰¹å¾é‡è¦æ€§:
```python
importance = |coef|
```

é‡è¦æ€§è¶Šé«˜,è¯¥ç‰¹å¾å¯¹é¢„æµ‹ç»“æœå½±å“è¶Šå¤§ã€‚

## ğŸ’¡ ä»£ç ç»“æ„

```python
load_data()                 # åŠ è½½å¤šç‰¹å¾CSVæ•°æ®
prepare_data()              # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
train_model()               # è®­ç»ƒæ¨¡å‹(å«æ ‡å‡†åŒ–)
evaluate_model()            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
visualize_results()         # å¯è§†åŒ–æ··æ·†çŸ©é˜µå’ŒROC
visualize_feature_importance()  # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
visualize_data_distribution()   # å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
main()                      # ä¸»å‡½æ•°
```

## ğŸ“ å­¦ä¹ è¦ç‚¹

1. **å¤šç‰¹å¾å»ºæ¨¡**: å¤„ç†å¤šä¸ªè¾“å…¥ç‰¹å¾
2. **ç‰¹å¾é¢„å¤„ç†**: æ ‡å‡†åŒ–çš„å¿…è¦æ€§
3. **ç‰¹å¾é€‰æ‹©**: è¯†åˆ«é‡è¦ç‰¹å¾
4. **æ¨¡å‹è§£é‡Š**: ç†è§£æ¯ä¸ªç‰¹å¾çš„ä½œç”¨
5. **æ•°æ®åˆ†å¸ƒåˆ†æ**: é€šè¿‡å¯è§†åŒ–ç†è§£æ•°æ®

## ğŸ“Š ä¸å•ç‰¹å¾ç‰ˆæœ¬å¯¹æ¯”

| ç‰¹æ€§ | å•ç‰¹å¾ç‰ˆæœ¬ | å¤šç‰¹å¾ç‰ˆæœ¬ |
|------|-----------|-----------|
| **ç‰¹å¾æ•°** | 1ä¸ª | 4ä¸ª |
| **æ•°æ®é‡** | 54æ ·æœ¬ | 61æ ·æœ¬ |
| **çœŸå®æ€§** | ç†æƒ³åŒ– | æ›´çœŸå® |
| **å‡†ç¡®æ€§** | ~85% | ~92% |
| **å¯è§†åŒ–** | å†³ç­–è¾¹ç•Œå›¾ | ç‰¹å¾é‡è¦æ€§å›¾ |
| **åº”ç”¨åœºæ™¯** | æ•™å­¦ç¤ºä¾‹ | å®é™…åº”ç”¨ |

## ğŸ”§ è‡ªå®šä¹‰æ•°æ®

### æ·»åŠ æ–°ç‰¹å¾

1. åœ¨CSVä¸­æ·»åŠ æ–°åˆ—:
```csv
å­¦ä¹ æ—¶é—´,å‡ºå‹¤ç‡,ä½œä¸šå®Œæˆç‡,è€ƒå‰å¤ä¹ ,è¯¾å¤–è¾…å¯¼,é€šè¿‡çŠ¶æ€
3.0,70,55,1.0,0.5,0
...
```

2. ä¿®æ”¹è„šæœ¬ä¸­çš„ç‰¹å¾åˆ—è¡¨:
```python
feature_columns = ['å­¦ä¹ æ—¶é—´(å°æ—¶)', 'å‡ºå‹¤ç‡(%)',
                  'ä½œä¸šå®Œæˆç‡(%)', 'è€ƒå‰å¤ä¹ (å°æ—¶)', 'è¯¾å¤–è¾…å¯¼(å°æ—¶)']
```

### è°ƒæ•´æ•°æ®é‡

ä¿®æ”¹CSVæ–‡ä»¶,å¢åŠ æˆ–å‡å°‘æ ·æœ¬æ•°ã€‚å»ºè®®:
- è®­ç»ƒé›†: è‡³å°‘40ä¸ªæ ·æœ¬
- æµ‹è¯•é›†: è‡³å°‘10ä¸ªæ ·æœ¬
- æ€»æ ·æœ¬: è¶Šå¤šè¶Šå¥½(å»ºè®®>50)

## ğŸ“š æ‰©å±•å»ºè®®

### 1. ç‰¹å¾äº¤äº’
```python
# åˆ›å»ºäº¤äº’ç‰¹å¾
data['å­¦ä¹ æ•ˆç‡'] = data['ä½œä¸šå®Œæˆç‡'] / data['å­¦ä¹ æ—¶é—´']
```

### 2. éçº¿æ€§ç‰¹å¾
```python
# æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
```

### 3. ç‰¹å¾é€‰æ‹©
```python
# ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤
from sklearn.feature_selection import RFE
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3)
rfe.fit(X_train, y_train)
```

### 4. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
```python
# è°ƒæ•´ç±»åˆ«æƒé‡
model = LogisticRegression(class_weight='balanced')
```

### 5. äº¤å‰éªŒè¯
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {scores.mean():.2f} (+/- {scores.std()*2:.2f})")
```

## âš ï¸ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆè¦æ ‡å‡†åŒ–ç‰¹å¾?**
A: ä¸åŒç‰¹å¾çš„é‡çº²å’ŒèŒƒå›´ä¸åŒ,æ ‡å‡†åŒ–ç¡®ä¿æ¯ä¸ªç‰¹å¾å¯¹æ¨¡å‹çš„è´¡çŒ®å…¬å¹³ã€‚

**Q: å¦‚ä½•åˆ¤æ–­å“ªä¸ªç‰¹å¾æœ€é‡è¦?**
A: æŸ¥çœ‹æ¨¡å‹ç³»æ•°çš„ç»å¯¹å€¼,è¶Šå¤§è¡¨ç¤ºå½±å“è¶Šå¤§ã€‚ä¹Ÿå¯ä»¥çœ‹ç‰¹å¾é‡è¦æ€§å›¾ã€‚

**Q: å‡†ç¡®ç‡ä¸ºä»€ä¹ˆæœ‰æ—¶ä¼šæ³¢åŠ¨?**
A: æ•°æ®éšæœºæ€§å¯¼è‡´ã€‚å¢åŠ æ•°æ®é‡æˆ–ä½¿ç”¨äº¤å‰éªŒè¯å¯ä»¥è·å¾—æ›´ç¨³å®šçš„è¯„ä¼°ã€‚

**Q: å¯ä»¥æ·»åŠ å¤šå°‘ä¸ªç‰¹å¾?**
A: ç†è®ºä¸Šæ— é™åˆ¶,ä½†è¦æ³¨æ„:
  - ç‰¹å¾è¿‡å¤šå¯èƒ½è¿‡æ‹Ÿåˆ
  - ç‰¹å¾æ•° << æ ·æœ¬æ•°
  - ä½¿ç”¨æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

**Q: è¾¹ç•Œæ¡ˆä¾‹å¦‚ä½•å¤„ç†?**
A: æ•°æ®ä¸­å·²åŒ…å«ä¸€äº›è¾¹ç•Œæ¡ˆä¾‹(å¦‚å­¦ä¹ æ—¶é—´çŸ­ä½†é€šè¿‡),è¿™äº›æ¡ˆä¾‹å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„å†³ç­–è¾¹ç•Œã€‚
