# 支持向量机(SVM)最小实例

一个完整的SVM示例,使用鸢尾花数据集进行品种分类,对比不同核函数的性能。

## 📁 文件说明

- **svm_sample.csv** - 鸢尾花数据文件(79个样本)
- **svm_example.py** - SVM操作脚本
- **svm_README.md** - 本说明文档

## 📊 数据说明

CSV文件包含5列(4个特征 + 1个目标):

### 特征变量
- `花瓣长度(cm)` - 花瓣长度: 1.3-5.0 cm
- `花瓣宽度(cm)` - 花瓣宽度: 0.2-1.8 cm
- `花萼长度(cm)` - 花萼长度: 4.3-7.0 cm
- `花萼宽度(cm)` - 花萼宽度: 2.0-4.4 cm

### 目标变量
- `品种` - 鸢尾花品种
  - 0 = 山鸢尾 (Setosa)
  - 1 = 维吉尼亚鸢尾 (Virginica)

### 数据特点
- ✅ **79个样本**: 充足的训练数据
- ✅ **4个特征**: 花瓣和花萼的尺寸
- ✅ **二分类**: 经典分类问题
- ✅ **线性可分**: 部分可分,需要核函数处理

## 🎯 什么是SVM?

**支持向量机(Support Vector Machine, SVM)**是一种强大的监督学习算法,主要用于:

### 核心思想
寻找一个最优超平面,使得两类数据点之间的**间隔(margin)**最大化。

### 关键概念

| 概念 | 说明 |
|------|------|
| **超平面** | 在高维空间中分割不同类别的决策边界 |
| **支持向量** | 距离决策边界最近的样本点 |
| **间隔** | 支持向量到超平面的距离 |
| **核函数** | 将数据映射到高维空间,使线性不可分数据变得可分 |

### 与其他算法对比

| 算法 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **SVM** | 高精度、鲁棒性强 | 大数据训练慢、参数敏感 | 中小规模、高维数据 |
| **逻辑回归** | 简单快速、易解释 | 线性边界限制 | 线性可分数据 |
| **决策树** | 可解释性强、无需特征缩放 | 容易过拟合 | 需要解释的场景 |

## 🚀 使用方法

### 1. 安装依赖

```bash
pip install pandas numpy scikit-learn matplotlib
```

### 2. 运行脚本

```bash
python svm_example.py
```

## 📈 输出内容

### 1. 控制台输出示例

```
支持向量机(SVM)示例 - 鸢尾花品种分类
======================================================================
数据预览:
   花瓣长度  花瓣宽度  花萼长度  花萼宽度  品种
0      1.4      0.2      5.1      3.5    0
1      1.3      0.3      4.9      3.0    0
...

品种分布:
品种
0    40
1    39

======================================================================
各核函数模型性能对比:
======================================================================

线性核 (Linear Kernel):
  准确率: 93.75%
  AUC值: 0.9896
  支持向量数: 14

RBF核 (RBF Kernel):
  准确率: 96.88%
  AUC值: 0.9948
  支持向量数: 12

多项式核 (Polynomial Kernel):
  准确率: 90.63%
  AUC值: 0.9792
  支持向量数: 18

Sigmoid核 (Sigmoid Kernel):
  准确率: 84.38%
  AUC值: 0.8958
  支持向量数: 22

======================================================================
示例预测:
======================================================================

样本1 (山鸢尾特征):
  花瓣长度: 1.5 cm
  花瓣宽度: 0.3 cm
  花萼长度: 5.0 cm
  花萼宽度: 3.5 cm
  预测品种: 山鸢尾
  山鸢尾概率: 98.23%
  维吉尼亚鸢尾概率: 1.77%
```

### 2. 可视化图表

#### svm_comparison.png (六合一图)

1. **左上 - 性能对比图**:
   - 蓝色柱: 各核函数的准确率
   - 红色柱: 各核函数的AUC值
   - 数值标签标注

2. **右上 - 右中 - 右下 - 各核函数的ROC曲线**:
   - RBF核(径向基函数): 通常效果最好
   - 线性核: 适合线性可分数据
   - 多项式核: 适合非线性关系
   - Sigmoid核: 类似神经网络

3. **底部 - 支持向量数量对比**:
   - 展示各核函数使用的支持向量数量
   - 支持向量越少,模型越简洁

#### svm_decision_boundary.png (决策边界可视化)

2×2子图展示不同核函数在2D特征空间(花瓣长度vs花瓣宽度)的决策边界:
- 背景色: 决策区域
- 红点: 山鸢尾
- 蓝点: 维吉尼亚鸢尾
- 黄色圆圈: 支持向量

#### svm_feature_distribution.png (特征分布)

2×2子图展示4个特征的分布情况:
- 红色直方图: 山鸢尾
- 蓝色直方图: 维吉尼亚鸢尾
- 可以看出哪些特征区分度最高

#### svm_concept.png (SVM原理图)

**左图 - 最大间隔原理**:
- 黑色实线: 决策边界
- 绿色虚线: 间隔边界
- 黄色圆圈: 支持向量
- 展示SVM如何最大化两类之间的间隔

**右图 - RBF核函数**:
- 展示径向基函数的形状
- 中心点相似度最高
- 距离越远相似度越低

## 🔑 核心概念详解

### 1. 超平面方程

**二分类SVM**:
```
w·x + b = 0
```
- w: 法向量(权重)
- b: 偏置
- x: 特征向量

**决策规则**:
```
如果 w·x + b > 0, 预测为类别1
如果 w·x + b < 0, 预测为类别0
```

### 2. 核函数

核函数将原始特征映射到高维空间,使线性不可分数据变得可分。

| 核函数 | 公式 | 特点 | 适用场景 |
|--------|------|------|----------|
| **线性核** | K(x,y) = x·y | 简单快速 | 线性可分数据 |
| **RBF核** | K(x,y) = exp(-γ||x-y||²) | 最常用 | 非线性可分 |
| **多项式核** | K(x,y) = (γx·y + r)ᵈ | 可控制阶数 | 需要特定非线性 |
| **Sigmoid核** | K(x,y) = tanh(γx·y + r) | 类似神经网络 | 特定场景 |

**RBF核详解**:
```python
from sklearn.svm import SVC

# RBF核有两个重要参数:
# C: 正则化参数,控制对误分类的惩罚
# gamma: 核系数,控制单个样本的影响范围

model = SVC(kernel='rbf', C=1.0, gamma='scale')
```

### 3. 软间隔与C参数

**硬间隔**:
- 要求所有样本都正确分类
- 对噪声敏感,容易过拟合

**软间隔**:
- 允许部分样本误分类
- 通过参数C控制权衡

```
C值大 → 严格要求正确分类 → 容易过拟合
C值小 → 允许更多误分类 → 可能欠拟合
```

### 4. 间隔最大化

**优化目标**:
```
最小化: ||w||²/2 + CΣξᵢ
约束条件: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ
```

- 第一项: 最大化间隔
- 第二项: 最小化误分类惩罚
- ξᵢ: 松弛变量,允许软间隔

## 💡 代码结构

```python
load_data()                    # 加载CSV数据
prepare_data()                 # 准备特征和目标变量
train_svm_models()             # 训练不同核函数的SVM
evaluate_model()               # 评估模型性能
visualize_comparison()         # 对比不同核函数
visualize_decision_boundary()  # 可视化决策边界
visualize_feature_distributions() # 特征分布图
visualize_svm_concept()        # SVM原理图
main()                         # 主函数
```

## 🎓 学习要点

1. **理解超平面**: 高维空间的决策边界
2. **掌握核函数**: 如何处理非线性可分数据
3. **参数调优**: C和gamma对模型的影响
4. **支持向量**: 理解哪些样本对决策边界重要
5. **多核对比**: 了解不同核函数的适用场景

## 🔧 参数调优指南

### C参数(正则化)

```python
# 小C值: 更大的间隔,更多的误分类
SVC(C=0.1)  # 容忍误分类,防止过拟合

# 大C值: 更小的间隔,更少的误分类
SVC(C=10)  # 严格要求正确分类

# 推荐范围: [0.1, 1, 10, 100]
```

### gamma参数(RBF核)

```python
# 小gamma: 影响范围大,决策边界平滑
SVC(gamma=0.01)  # 适合大数据集

# 大gamma: 影响范围小,决策边界复杂
SVC(gamma=10)  # 适合小数据集

# 推荐使用:
SVC(gamma='scale')  # 自动: 1/(n_features * X.var())
SVC(gamma='auto')   # 自动: 1/n_features
```

### 核函数选择流程图

```
开始
  ↓
数据是否线性可分?
  ↓ 是              ↓ 否
使用线性核        数据量是否很大?
  ↓                ↓ 是                ↓ 否
                  使用线性核         尝试RBF核
  ↓                                        ↓
                                          特征是否很多?
  ↓                    ↓ 是              ↓ 否
                      使用线性核       尝试多项式核
  ↓                                        ↓
                                    效果是否提升?
  ↓                    ↓ 否              ↓ 是
                    继续使用RBF核      使用多项式核
```

## 📚 扩展应用

### 1. 多分类SVM

```python
# One-vs-One策略
from sklearn.multiclass import OneVsOneClassifier
ovo = OneVsOneClassifier(SVC(kernel='rbf'))

# One-vs-Rest策略
from sklearn.multiclass import OneVsRestClassifier
ovr = OneVsRestClassifier(SVC(kernel='rbf'))
```

### 2. 概率预测

```python
# 启用概率估计
model = SVC(kernel='rbf', probability=True)
model.fit(X_train, y_train)
proba = model.predict_proba(X_test)  # 获得概率
```

### 3. 特征缩放的重要性

```python
from sklearn.preprocessing import StandardScaler

# SVM对特征尺度敏感,必须标准化!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### 4. 处理不平衡数据

```python
# 自动调整类别权重
model = SVC(kernel='rbf', class_weight='balanced')

# 手动设置权重
model = SVC(kernel='rbf', class_weight={0: 1, 1: 2})
```

### 5. 网格搜索最优参数

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1, 1],
    'kernel': ['rbf', 'linear']
}

grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train, y_train)

print(f"最佳参数: {grid.best_params_}")
print(f"最佳分数: {grid.best_score_}")
```

## ⚠️ 常见问题

**Q: SVM为什么叫"支持向量机"?**
A: 因为训练过程依赖于"支持向量",即距离决策边界最近的那些样本点。

**Q: 什么时候使用线性核?**
A: 数据线性可分、特征数很多、样本量很大时。

**Q: RBF核的gamma参数怎么选?**
A: 通常从'scale'开始,如果过拟合则减小,欠拟合则增大。

**Q: SVM能处理大数据吗?**
A: 训练时间复杂度为O(n²)到O(n³),大数据(>10万样本)训练很慢,建议使用逻辑回归或随机森林。

**Q: 如何判断模型是否过拟合?**
A: 训练集准确率远高于测试集,增加C值或减小gamma值可缓解。

**Q: SVM需要特征标准化吗?**
A: **必须!** SVM对特征尺度非常敏感,不标准化会导致某些特征主导决策边界。

**Q: 支持向量越少越好吗?**
A: 通常是的,支持向量少说明模型更简洁、泛化能力更强。

## 📊 算法复杂度对比

| 指标 | SVM | 逻辑回归 | 决策树 |
|------|-----|----------|--------|
| 训练时间 | O(n²)-O(n³) | O(n) | O(n log n) |
| 预测时间 | O(k) | O(p) | O(depth) |
| 内存占用 | O(k) | O(p) | O(nodes) |
| 适合数据规模 | 中小规模 | 任意规模 | 任意规模 |

- n: 样本数
- k: 支持向量数
- p: 特征数
- depth: 树深度

## 🎯 实践建议

1. **数据预处理**: 必须标准化特征
2. **参数选择**: 从默认值开始,网格搜索调优
3. **核函数**: 优先尝试RBF核
4. **评估**: 使用交叉验证避免过拟合
5. **可视化**: 对低维数据可视化决策边界
