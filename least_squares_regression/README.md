# 最小二乘回归算法示例

## 简介

本示例展示如何使用最小二乘法(Least Squares)进行线性回归，具体场景是**房价预测**。

最小二乘法是回归分析中最基础也是最重要的方法，通过最小化预测值与真实值之间的误差平方和来求解模型参数。

## 数据集

使用生成的模拟房价数据，包含以下特征：

- **面积**: 房屋面积 (30-200平方米)
- **卧室数**: 卧室数量 (1-5个)
- **房龄**: 房屋年龄 (0-30年)
- **房价**: 目标变量，单位万元

数据集大小: 500个样本
房价范围: 46.02 - 410.02 万元

## 文件说明

```
least_squares_regression/
├── least_squares_example.py    # 主程序
├── least_squares_sample.csv    # 数据集
├── generate_data.py            # 数据生成脚本
└── README.md                   # 本文档
```

## 依赖安装

```bash
pip install numpy pandas scikit-learn matplotlib seaborn scipy
```

可选: 用于生成GIF动画
```bash
pip install Pillow
```

## 使用方法

### 1. 运行完整示例

```bash
python least_squares_example.py
```

### 2. 生成新的数据集

```bash
python generate_data.py
```

## 程序功能

### 1. 数据加载与准备
- 从CSV文件加载房价数据
- 划分训练集和测试集 (70%/30%)
- 特征标准化（用于梯度下降）

### 2. 模型训练

#### 正规方程法
```python
θ = (X^T * X)^(-1) * X^T * y
```
- 直接求解最优参数
- 适合小规模数据
- 计算复杂度: O(n³)

#### 梯度下降法
```python
θ := θ - α * ∇J(θ)
```
- 迭代优化参数
- 适合大规模数据
- 可以观察优化过程

### 3. 模型评估
- **MSE**: 均方误差
- **R²**: 决定系数（解释方差比例）
- **MAE**: 平均绝对误差

### 4. 可视化

程序会生成以下可视化图表：

1. **回归线图** (`ls_regression_line.png`)
   - 展示每个特征与房价的关系
   - 绘制回归线和散点图

2. **残差分析图** (`ls_residuals.png`)
   - 残差 vs 预测值散点图
   - 残差分布直方图
   - Q-Q图（正态性检验）
   - 残差顺序图

3. **损失函数曲面** (`ls_loss_surface.png`)
   - 3D曲面图展示损失函数
   - 等高线图展示最优解位置

4. **几何意义图** (`ls_geometry.png`)
   - 3D回归平面可视化
   - 向量投影的几何解释
   - 正规方程的数学推导

5. **模型对比图** (`ls_model_comparison.png`)
   - 训练集 vs 测试集预测效果
   - 评估指标对比柱状图
   - 回归系数解释

6. **最小二乘回归动画** (`least_squares_animation.gif`)
   - 动态展示梯度下降优化过程

## 动画说明

最小二乘回归动画展示了梯度下降的优化过程：

- **左上图**: 回归线演化
  - 蓝色点: 实际数据
  - 红色线: 当前回归线
  - 绿色虚线: 最优回归线
  - 橙色线: 残差（误差）

- **右上图**: 损失函数下降曲线
  - 展示MSE随迭代次数的变化
  - 红色星: 当前损失值

- **左下图**: 参数演化轨迹
  - 蓝色线: 截距θ₀的变化
  - 红色线: 斜率θ₁的变化

- **右下图**: 模型统计信息
  - 当前参数值和最优参数对比
  - 模型性能指标
  - 损失降幅百分比

## 最小二乘法原理

### 1. 数学模型

假设我们有n个样本，每个样本有p个特征：

```
y = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₚxₚ + ε
```

矩阵形式：
```
y = Xθ + ε
```

其中：
- y: n×1的目标向量
- X: n×(p+1)的特征矩阵（包含全1列）
- θ: (p+1)×1的参数向量
- ε: 误差向量

### 2. 损失函数

最小二乘法的目标是最小化误差平方和：

```
J(θ) = Σ(yᵢ - ŷᵢ)² = ||y - Xθ||²
```

### 3. 正规方程推导

对损失函数求导并令其为0：

```
∇θJ(θ) = -2Xᵀ(y - Xθ) = 0
XᵀXθ = Xᵀy
θ = (XᵀX)⁻¹Xᵀy
```

这就是**正规方程**，可以直接求得最优参数。

### 4. 几何意义

从几何角度看：
- y向量可以分解为两个正交分量：
  - Xθ: 在X列空间上的投影
  - ε: 垂直于X列空间的残差

最小二乘法保证：
- 残差向量垂直于X的列空间
- Xθ是y在X列空间上的最佳逼近

### 5. 梯度下降法

当特征数量很多时，正规方程计算代价高，可以使用梯度下降：

```python
repeat until convergence {
    θⱼ := θⱼ - α * ∂J(θ)/∂θⱼ
}
```

其中梯度为：
```
∇θJ(θ) = (2/m) * Xᵀ(Xθ - y)
```

## 算法特点

### 优点
- ✅ 简单直观，易于理解和实现
- ✅ 有解析解（正规方程）
- ✅ 可解释性强
- ✅ 计算效率高（小规模数据）

### 缺点
- ❌ 对异常值敏感
- ❌ 假设线性关系（可能欠拟合）
- ❌ 特征相关时会导致多重共线性

### 适用场景
- 特征数量 < 10000
- 特征之间相关性不高
- 需要可解释的模型
- 快速建立baseline

## 参数调优建议

1. **学习率** (梯度下降)
   - 默认: 0.01
   - 太小: 收敛慢
   - 太大: 可能不收敛
   - 建议: 0.001 - 0.1

2. **迭代次数** (梯度下降)
   - 默认: 1000
   - 可通过损失曲线判断是否收敛
   - 建议: 500 - 10000

3. **特征工程**
   - 特征标准化：使梯度下降更稳定
   - 特征选择：去除相关特征
   - 多项式特征：捕获非线性关系

## 输出示例

```
最小二乘回归算法示例 - 房价预测
======================================================================

步骤1: 加载数据
======================================================================
数据加载成功: least_squares_sample.csv
数据形状: (500, 4)

步骤2: 准备数据
======================================================================
特征列: ['面积', '卧室数', '房龄']
样本数量: 500
房价范围: 46.02 - 410.02 万元

步骤3: 划分训练集和测试集
======================================================================
训练集大小: 350
测试集大小: 150

步骤4: 使用正规方程训练模型
======================================================================
模型训练完成!
回归系数:
  截距 (θ₀): 19.8234
  面积 (θ₁): 1.5123
  卧室数 (θ₂): 14.8765
  房龄 (θ₃): -0.7891

步骤6: 评估模型
======================================================================
训练集:
  MSE: 224.5678
  R²: 0.8234
  MAE: 11.2345

测试集:
  MSE: 245.6789
  R²: 0.8012
  MAE: 12.3456
```

## 扩展阅读

- [普通最小二乘法 - Wikipedia](https://zh.wikipedia.org/wiki/普通最小二乘法)
- [线性回归 - Scikit-learn文档](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)
- [梯度下降算法详解](https://blog.csdn.net/qq_41800366/article/details/86582289)

## 与其他算法的关系

| 特性 | 最小二乘回归 | 岭回归 | Lasso回归 |
|------|------------|--------|----------|
| 正则化 | 无 | L2正则化 | L1正则化 |
| 解析解 | ✅ | ✅ | ❌ |
| 特征选择 | ❌ | ❌ | ✅ |
| 多重共线性 | 敏感 | 处理 | 处理 |

## 实际应用

1. **经济学**: 需求预测、价格分析
2. **金融学**: 股票收益预测、风险评估
3. **医学**: 疾病诊断、药效评估
4. **工程学**: 质量控制、性能预测
5. **市场营销**: 销售预测、客户分析

## 注意事项

1. **数据预处理**
   - 特征标准化（特别是梯度下降）
   - 处理缺失值
   - 异常值检测

2. **模型诊断**
   - 检查残差是否正态分布
   - 验证线性假设
   - 检测多重共线性

3. **结果解释**
   - 系数的符号应符合业务逻辑
   - R²不是唯一标准
   - 考虑业务场景的误差容忍度

## 许可证

MIT License
